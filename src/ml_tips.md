# æ©Ÿæ¢°å­¦ç¿’Tips

ã‚ˆãä½¿ã†ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é›†ã§ã™ã€‚

## ğŸ“š åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³

### 1. pandas ã§å‰å‡¦ç†

```python
import pandas as pd
import numpy as np

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('../input/titanic/train.csv')

# æ¬ æå€¤ã®ç¢ºèª
print(df.isnull().sum())

# æ¬ æå€¤ã®è£œå®Œï¼ˆä¸­å¤®å€¤ï¼‰
df['Age'] = df['Age'].fillna(df['Age'].median())

# æ¬ æå€¤ã®è£œå®Œï¼ˆæœ€é »å€¤ï¼‰
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])

# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆOne-Hotï¼‰
df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)

# æ•°å€¤ç‰¹å¾´é‡ã®é¸æŠ
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
```

### 2. scikit-learn ã§å­¦ç¿’

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
model = RandomForestClassifier(
    n_estimators=200,
    max_depth=5,
    random_state=42
)
model.fit(X_train, y_train)

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred = model.predict(X_valid)
print('Accuracy:', accuracy_score(y_valid, y_pred))
print(classification_report(y_valid, y_pred))
```

### 3. matplotlib/seaborn ã§å¯è¦–åŒ–

```python
import matplotlib.pyplot as plt
import seaborn as sns

# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)

# ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒ
sns.countplot(x=y)
plt.title('Target Distribution')
plt.show()

# ç‰¹å¾´é‡é‡è¦åº¦
importances = pd.Series(model.feature_importances_, index=X.columns)
importances.sort_values().plot(kind='barh')
plt.title('Feature Importances')
plt.show()

# ç›¸é–¢è¡Œåˆ—
corr = df.corr()
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
plt.show()
```

## ğŸš€ é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³

### LightGBMï¼ˆåˆ†é¡ï¼‰

```python
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# å­¦ç¿’çµŒéã‚’è¨˜éŒ²
evals_result = {}

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
gbm = lgb.LGBMClassifier(
    objective='binary',
    importance_type='gain',
    n_estimators=1000
)

# å­¦ç¿’
gbm.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_valid, y_valid)],
    eval_names=['train', 'valid'],
    eval_metric='binary_logloss',
    callbacks=[
        lgb.early_stopping(20),      # 20å›æ”¹å–„ã—ãªã‘ã‚Œã°æ‰“ã¡åˆ‡ã‚Š
        lgb.log_evaluation(0),       # ãƒ­ã‚°ä¸è¦ãªã‚‰0
        lgb.record_evaluation(evals_result),  # çµæœã‚’è¨˜éŒ²
    ],
)

# lossæ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ
plt.figure(figsize=(8, 4))
plt.plot(evals_result['train']['binary_logloss'], label='train_loss')
plt.plot(evals_result['valid']['binary_logloss'], label='valid_loss')
plt.xlabel('Iteration')
plt.ylabel('binary_logloss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# äºˆæ¸¬
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)
```

### äº¤å·®æ¤œè¨¼ï¼ˆKFoldï¼‰

```python
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
import numpy as np

kf = KFold(n_splits=5, shuffle=True, random_state=42)
score_list = []
models = []

for fold, (train_index, valid_index) in enumerate(kf.split(X, y)):
    X_train_fold = X.iloc[train_index]
    X_valid_fold = X.iloc[valid_index]
    y_train_fold = y.iloc[train_index]
    y_valid_fold = y.iloc[valid_index]
    
    print(f'Fold {fold + 1} start')
    
    model = RandomForestClassifier(n_estimators=200, random_state=42)
    model.fit(X_train_fold, y_train_fold)
    
    y_pred = model.predict(X_valid_fold)
    score = accuracy_score(y_valid_fold, y_pred)
    score_list.append(score)
    models.append(model)
    
    print(f'Fold {fold + 1} score: {score:.4f}')

print(f'Average score: {np.mean(score_list):.4f}')
```

### æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ

```python
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
test_pred = model.predict(X_test)

# æå‡ºç”¨DataFrameä½œæˆ
# æ³¨æ„: testãƒ‡ãƒ¼ã‚¿ã‹ã‚‰PassengerIdã‚’å–å¾—ï¼ˆå‰å‡¦ç†ã§å‰Šé™¤ã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
submission = pd.DataFrame({
    'PassengerId': test['PassengerId'],  # å…ƒã®testãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—
    'Survived': test_pred
})

# CSVã¨ã—ã¦ä¿å­˜
submission.to_csv('../submissions/submission.csv', index=False)
print(submission.head())
```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆ`OSError: [Errno 35] Resource deadlock avoided`ï¼‰

**åŸå› **: pandasã®Cã‚¨ãƒ³ã‚¸ãƒ³ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®ç›¸æ€§å•é¡Œ

**è§£æ±ºç­–1**: Pythonã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
```python
pd.read_csv('path/to/file.csv', engine='python')
```

**è§£æ±ºç­–2**: csvãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨
```python
import csv
import pandas as pd

rows = []
with open('path/to/file.csv', 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    for r in reader:
        rows.append(r)
df = pd.DataFrame(rows)
```

### LightGBMã®æ—©æœŸåœæ­¢ã‚¨ãƒ©ãƒ¼

**ã‚¨ãƒ©ãƒ¼**: `TypeError: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'`

**åŸå› **: æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ `early_stopping_rounds` ãŒéæ¨å¥¨

**è§£æ±ºç­–**: `callbacks` ã‚’ä½¿ç”¨
```python
# å¤ã„æ›¸ãæ–¹ï¼ˆéæ¨å¥¨ï¼‰
# gbm.fit(..., early_stopping_rounds=20)

# æ–°ã—ã„æ›¸ãæ–¹
gbm.fit(
    X_train, y_train,
    eval_set=[(X_valid, y_valid)],
    callbacks=[
        lgb.early_stopping(20),
        lgb.log_evaluation(0)
    ]
)
```

### æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã§ `KeyError: 'PassengerId'`

**åŸå› **: å‰å‡¦ç†ã§ `PassengerId` ãŒå‰Šé™¤ã•ã‚ŒãŸ

**è§£æ±ºç­–**: å…ƒã®testãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿
```python
# å…ƒã®testãƒ‡ãƒ¼ã‚¿ã‚’å†èª­ã¿è¾¼ã¿
test_org = pd.read_csv('../input/titanic/test.csv')
submission = pd.DataFrame({
    'PassengerId': test_org['PassengerId'],
    'Survived': test_pred
})
```

### `NameError: name 'X_test' is not defined`

**åŸå› **: `X_test` ãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„

**è§£æ±ºç­–**: ç‰¹å¾´é‡ã‚’æ˜ç¤ºçš„ã«å®šç¾©
```python
# ç‰¹å¾´é‡ã®å®šç¾©
feature_cols = [c for c in train.columns if c not in ['PassengerId', 'Survived']]
X_train = train[feature_cols]
X_test = test[feature_cols]  # ã“ã‚ŒãŒå¿…è¦
```

### `DeprecationWarning: import pandas_profiling`

**åŸå› **: `pandas_profiling` ãŒéæ¨å¥¨

**è§£æ±ºç­–**: `ydata_profiling` ã‚’ä½¿ç”¨
```python
# å¤ã„æ›¸ãæ–¹
# from pandas_profiling import ProfileReport

# æ–°ã—ã„æ›¸ãæ–¹
from ydata_profiling import ProfileReport
```

## ğŸ’¡ ã‚ˆãä½¿ã†ä¾¿åˆ©ã‚³ãƒ¼ãƒ‰

### ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±ã‚’ä¸€æ‹¬è¡¨ç¤º

```python
# pandasã®è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
pd.set_option('display.float_format', lambda x: f'{x:,.2f}')
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

# åŸºæœ¬æƒ…å ±
train_df.info()

# çµ±è¨ˆã‚µãƒãƒªãƒ¼ï¼ˆå·¦æƒãˆï¼‰
stats_df = pd.DataFrame(train_df[target_col].describe(), columns=[target_col])
display(stats_df.style.set_properties(**{'text-align': 'left'}))
```

### æ¬ æå€¤ã®å¯è¦–åŒ–

```python
# æ¬ æå€¤ã®å¤šã„ç‰¹å¾´é‡ï¼ˆä¸Šä½10å€‹ï¼‰
missing = train_df.isnull().sum().sort_values(ascending=False).head(10)
if missing.sum() > 0:
    missing[missing > 0].plot(kind='barh', figsize=(8, 6))
    plt.title('æ¬ æå€¤ã®å¤šã„ç‰¹å¾´é‡ï¼ˆä¸Šä½10å€‹ï¼‰')
    plt.show()
```

### ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–

```python
importances = pd.Series(model.feature_importances_, index=X.columns)
importances.sort_values(ascending=False).head(20).plot(kind='barh', figsize=(10, 8))
plt.title('Feature Importances (Top 20)')
plt.xlabel('Importance')
plt.tight_layout()
plt.show()
```

---

**é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [README.md](../README.md) | [Notebookã®èª¬æ˜](notebooks.md)

