{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices: Advanced Regression Techniques - åˆå¿ƒè€…å‘ã‘ã‚¬ã‚¤ãƒ‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€House Pricesã‚³ãƒ³ãƒšã«å–ã‚Šçµ„ã¿ãªãŒã‚‰ã€**å›å¸°å•é¡Œ**ã®æ©Ÿæ¢°å­¦ç¿’ã‚’å­¦ã³ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“š ã“ã®ã‚³ãƒ³ãƒšã«ã¤ã„ã¦\n",
    "\n",
    "**å•é¡Œã®ç¨®é¡**: å›å¸°ï¼ˆRegressionï¼‰\n",
    "- **å›å¸°ã¨ã¯**: é€£ç¶šã—ãŸæ•°å€¤ã‚’äºˆæ¸¬ã™ã‚‹å•é¡Œã§ã™\n",
    "- ã“ã®ã‚³ãƒ³ãƒšã§ã¯ã€ä½å®…ã®ç‰¹å¾´ï¼ˆé¢ç©ã€éƒ¨å±‹æ•°ã€ç«‹åœ°ãªã©ï¼‰ã‹ã‚‰**ä½å®…ä¾¡æ ¼**ã‚’äºˆæ¸¬ã—ã¾ã™\n",
    "- åˆ†é¡å•é¡Œï¼ˆ0ã‹1ï¼‰ã¨ã¯ç•°ãªã‚Šã€**é€£ç¶šã—ãŸå€¤**ï¼ˆä¾‹: $150,000, $200,000ãªã©ï¼‰ã‚’äºˆæ¸¬ã—ã¾ã™\n",
    "\n",
    "**è©•ä¾¡æŒ‡æ¨™**: RMSEï¼ˆRoot Mean Squared Error / å¹³å‡äºŒä¹—èª¤å·®ã®å¹³æ–¹æ ¹ï¼‰\n",
    "- **RMSEã¨ã¯**: äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å€¤ã®å·®ã‚’æ¸¬ã‚‹æŒ‡æ¨™ã§ã™\n",
    "- å€¤ãŒå°ã•ã„ã»ã©è‰¯ã„ï¼ˆå®Œç’§ãªäºˆæ¸¬ãªã‚‰0ï¼‰\n",
    "- å˜ä½ã¯ç›®çš„å¤‰æ•°ã¨åŒã˜ï¼ˆã“ã®å ´åˆã€ãƒ‰ãƒ«ï¼‰\n",
    "\n",
    "**ãƒ‡ãƒ¼ã‚¿å½¢å¼**: è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ï¼ˆTabular Dataï¼‰\n",
    "- CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚ˆã†ãªè¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿\n",
    "- å„è¡ŒãŒ1ã¤ã®ä½å®…ã€å„åˆ—ãŒç‰¹å¾´ï¼ˆé¢ç©ã€éƒ¨å±‹æ•°ãªã©ï¼‰ã‚’è¡¨ã—ã¾ã™\n",
    "\n",
    "## ğŸ”„ åˆ†é¡å•é¡Œã¨ã®é•ã„\n",
    "\n",
    "| é …ç›® | åˆ†é¡å•é¡Œï¼ˆTitanicãªã©ï¼‰ | å›å¸°å•é¡Œï¼ˆHouse Pricesï¼‰ |\n",
    "|------|----------------------|----------------------|\n",
    "| äºˆæ¸¬ã™ã‚‹å€¤ | ã‚«ãƒ†ã‚´ãƒªï¼ˆ0ã‹1ï¼‰ | é€£ç¶šã—ãŸæ•°å€¤ï¼ˆä¾¡æ ¼ãªã©ï¼‰ |\n",
    "| è©•ä¾¡æŒ‡æ¨™ | Log Loss, Accuracy | RMSE, MAE |\n",
    "| ãƒ¢ãƒ‡ãƒ« | åˆ†é¡å™¨ | å›å¸°å™¨ |\n",
    "| ä¾‹ | ç”Ÿå­˜ã—ãŸã‹ã©ã†ã‹ | ä½å®…ä¾¡æ ¼ã¯ã„ãã‚‰ã‹ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å›å¸°å•é¡Œã®åŸºæœ¬ç”¨èª\n",
    "\n",
    "### é‡è¦ãªç”¨èªé›†\n",
    "\n",
    "1. **å›å¸°ï¼ˆRegressionï¼‰**\n",
    "   - é€£ç¶šã—ãŸæ•°å€¤ã‚’äºˆæ¸¬ã™ã‚‹å•é¡Œ\n",
    "   - ä¾‹: ä¾¡æ ¼ã€æ¸©åº¦ã€å£²ä¸Šãªã©\n",
    "\n",
    "2. **RMSEï¼ˆRoot Mean Squared Errorï¼‰**\n",
    "   - äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å€¤ã®å·®ã‚’æ¸¬ã‚‹æŒ‡æ¨™\n",
    "   - å¼: âˆš(å¹³å‡((äºˆæ¸¬å€¤ - å®Ÿéš›ã®å€¤)Â²))\n",
    "   - å¤–ã‚Œå€¤ï¼ˆå¤§ããå¤–ã‚ŒãŸäºˆæ¸¬ï¼‰ã«æ•æ„Ÿ\n",
    "\n",
    "3. **MAEï¼ˆMean Absolute Errorï¼‰**\n",
    "   - äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å€¤ã®å·®ã®çµ¶å¯¾å€¤ã®å¹³å‡\n",
    "   - å¤–ã‚Œå€¤ã«æ¯”è¼ƒçš„å¼·ã„\n",
    "\n",
    "4. **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**\n",
    "   - ã“ã®ã‚³ãƒ³ãƒšã§ã¯ç‰¹ã«é‡è¦ï¼\n",
    "   - æ—¢å­˜ã®ç‰¹å¾´é‡ã‹ã‚‰æ–°ã—ã„ç‰¹å¾´é‡ã‚’ä½œã‚‹\n",
    "   - ä¾‹: ç·é¢ç© = 1éšã®é¢ç© + 2éšã®é¢ç©\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š å…¨ä½“ã®æµã‚Œï¼ˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼‰\n",
    "\n",
    "```\n",
    "1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "   â†“\n",
    "2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "   â†“\n",
    "3. ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ï¼ˆEDAï¼‰\n",
    "   - ç›®çš„å¤‰æ•°ï¼ˆä¾¡æ ¼ï¼‰ã®åˆ†å¸ƒã‚’ç¢ºèª\n",
    "   - ç‰¹å¾´é‡ã®åˆ†å¸ƒã€æ¬ æå€¤ã‚’ç¢ºèª\n",
    "   â†“\n",
    "4. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆé‡è¦ï¼ï¼‰\n",
    "   - æ–°ã—ã„ç‰¹å¾´é‡ã®ä½œæˆ\n",
    "   - ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®å‡¦ç†\n",
    "   â†“\n",
    "5. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†\n",
    "   - æ¬ æå€¤ã®å‡¦ç†\n",
    "   - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "   â†“\n",
    "6. ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
    "   - ç·šå½¢å›å¸°ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãªã©\n",
    "   â†“\n",
    "7. äºˆæ¸¬ã®ç”Ÿæˆ\n",
    "   - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’å®Ÿè¡Œ\n",
    "   â†“\n",
    "8. æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "   - Kaggleæå‡ºç”¨ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "å›å¸°å•é¡Œç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import pandas as pd  # ãƒ‡ãƒ¼ã‚¿æ“ä½œ\n",
    "import numpy as np   # æ•°å€¤è¨ˆç®—\n",
    "import matplotlib.pyplot as plt  # ã‚°ãƒ©ãƒ•æç”»\n",
    "import seaborn as sns  # ã‚ˆã‚Šè¦‹ã‚„ã™ã„ã‚°ãƒ©ãƒ•\n",
    "import warnings  # è­¦å‘Šã‚’éè¡¨ç¤ºã«ã™ã‚‹\n",
    "\n",
    "# æ©Ÿæ¢°å­¦ç¿’é–¢é€£ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆå›å¸°ç”¨ï¼‰\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# EDAè‡ªå‹•åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "try:\n",
    "    from ydata_profiling import ProfileReport\n",
    "    EDA_AVAILABLE = True\n",
    "    print(\"âœ… ydata-profiling ãŒåˆ©ç”¨å¯èƒ½ã§ã™\")\n",
    "except ImportError:\n",
    "    EDA_AVAILABLE = False\n",
    "    print(\"âš ï¸ ydata-profiling ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "\n",
    "# è­¦å‘Šã‚’éè¡¨ç¤ºã«ã™ã‚‹\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ã‚°ãƒ©ãƒ•ã®ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "### ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹æ³•\n",
    "\n",
    "1. **Kaggle APIã‚’ä½¿ã†å ´åˆ**:\n",
    "   ```bash\n",
    "   kaggle competitions download -c house-prices-advanced-regression-techniques -p input/\n",
    "   cd input/\n",
    "   unzip house-prices-advanced-regression-techniques.zip\n",
    "   ```\n",
    "\n",
    "2. **Kaggleã®Webã‚µã‚¤ãƒˆã‹ã‚‰æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**:\n",
    "   - ã‚³ãƒ³ãƒšãƒšãƒ¼ã‚¸ã®ã€ŒDataã€ã‚¿ãƒ–ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "   - `input/house-prices-advanced-regression-techniques/` ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜\n",
    "\n",
    "### ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦\n",
    "\n",
    "- `train.csv`: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆç‰¹å¾´é‡ + ä½å®…ä¾¡æ ¼ï¼‰\n",
    "- `test.csv`: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆç‰¹å¾´é‡ã®ã¿ï¼‰\n",
    "- `sample_submission.csv`: æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ä¾‹\n",
    "- `data_description.txt`: å„ç‰¹å¾´é‡ã®èª¬æ˜ï¼ˆé‡è¦ï¼ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "# æ³¨æ„: Dockerç’°å¢ƒã§ã¯ ../input/ ã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™\n",
    "# æ•´ç†å¾Œã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ : input/house-prices-advanced-regression-techniques/\n",
    "\n",
    "try:\n",
    "    # ã¾ãšãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒï¼ˆDockerï¼‰ã®ãƒ‘ã‚¹ã‚’è©¦ã™ï¼ˆæ•´ç†å¾Œã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ï¼‰\n",
    "    train_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "    test_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "    print(\"âœ… ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒï¼ˆæ•´ç†å¾Œã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        # Kaggleç’°å¢ƒã®å ´åˆ\n",
    "        train_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
    "        test_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n",
    "        print(\"âœ… Kaggleç’°å¢ƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "    except FileNotFoundError:\n",
    "        # åˆ¥ã®ãƒ‘ã‚¹ã‚’è©¦ã™\n",
    "        train_df = pd.read_csv('input/house-prices-advanced-regression-techniques/train.csv')\n",
    "        test_df = pd.read_csv('input/house-prices-advanced-regression-techniques/test.csv')\n",
    "        print(\"âœ… åˆ¥ã®ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±ã‚’ç¢ºèª\n",
    "print(f\"\n",
    "ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚º: {train_df.shape}\")\n",
    "print(f\"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚º: {test_df.shape}\")\n",
    "print(f\"\n",
    "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œ:\")\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ï¼ˆEDAï¼‰\n",
    "\n",
    "å›å¸°å•é¡Œã§ã¯ã€**ç›®çš„å¤‰æ•°ï¼ˆä¾¡æ ¼ï¼‰ã®åˆ†å¸ƒ**ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒç‰¹ã«é‡è¦ã§ã™ã€‚\n",
    "ä¾¡æ ¼ãŒæ­£è¦åˆ†å¸ƒã«è¿‘ã„ã‹ã€å¤–ã‚Œå€¤ãŒã‚ã‚‹ã‹ãªã©ã‚’ç¢ºèªã—ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›®çš„å¤‰æ•°ï¼ˆSalePriceï¼‰ã®ç¢ºèªï¼ˆæœ€ã‚‚ç°¡å˜ãªå®Ÿè£…ï¼‰\n",
    "target_col = 'SalePrice'\n",
    "\n",
    "if target_col in train_df.columns:\n",
    "    # åŸºæœ¬çµ±è¨ˆ\n",
    "    print(train_df[target_col].describe())\n",
    "    print(f\"\\nğŸ’¡ æ­ªåº¦: {train_df[target_col].skew():.2f}\")\n",
    "    \n",
    "    # pandasã®plotãƒ¡ã‚½ãƒƒãƒ‰ã§1è¡Œã§å¯è¦–åŒ–\n",
    "    train_df[target_col].hist(bins=50, figsize=(10, 5), edgecolor='black')\n",
    "    plt.title('ä½å®…ä¾¡æ ¼ã®åˆ†å¸ƒ')\n",
    "    plt.xlabel('ä¾¡æ ¼ï¼ˆãƒ‰ãƒ«ï¼‰')\n",
    "    plt.ylabel('ä»¶æ•°')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ ç›®çš„å¤‰æ•° 'SalePrice' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ç°¡æ½”ãªEDA: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ãŸä¸€æ‹¬å¯è¦–åŒ–\n",
    "\n",
    "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã§ã€ä¸»è¦ãªEDAã‚’ä¸€åº¦ã«å®Ÿè¡Œã§ãã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡æ½”ãªEDA: åŸºæœ¬æƒ…å ±ã¨æ¬ æå€¤ï¼ˆæœ€ã‚‚ç°¡å˜ãªå®Ÿè£…ï¼‰\n",
    "# pandasã®info()ã§åŸºæœ¬æƒ…å ±ã‚’ä¸€æ‹¬è¡¨ç¤º\n",
    "train_df.info()\n",
    "\n",
    "# æ¬ æå€¤ã®å¤šã„ç‰¹å¾´é‡ï¼ˆä¸Šä½10å€‹ï¼‰ã‚’å¯è¦–åŒ–\n",
    "missing = train_df.isnull().sum().sort_values(ascending=False).head(10)\n",
    "if missing.sum() > 0:\n",
    "    missing[missing > 0].plot(kind='barh', figsize=(8, 6))\n",
    "    plt.title('æ¬ æå€¤ã®å¤šã„ç‰¹å¾´é‡ï¼ˆä¸Šä½10å€‹ï¼‰')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›®çš„å¤‰æ•°ã¨é‡è¦ãªç‰¹å¾´é‡ã®ä¸€æ‹¬å¯è¦–åŒ–ï¼ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆç‰ˆï¼‰\n",
    "target_col = 'SalePrice'\n",
    "\n",
    "if target_col in train_df.columns:\n",
    "    # æ•°å€¤ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆIdã¨ç›®çš„å¤‰æ•°ã‚’é™¤å¤–ï¼‰\n",
    "    numeric_cols = [c for c in train_df.select_dtypes(include=[np.number]).columns \n",
    "                    if c not in ['Id', target_col]]\n",
    "    \n",
    "    # ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆä¸Šä½4å€‹ï¼‰\n",
    "    top_features = train_df[numeric_cols + [target_col]].corr()[target_col].abs().sort_values(ascending=False).head(5).index.tolist()\n",
    "    top_features = [f for f in top_features if f != target_col][:4]\n",
    "    \n",
    "    # å¯è¦–åŒ–ï¼ˆ2è¡Œ2åˆ—ï¼‰\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # ç›®çš„å¤‰æ•°ã®åˆ†å¸ƒ\n",
    "    axes[0].hist(train_df[target_col], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_title(f'{target_col}ã®åˆ†å¸ƒ')\n",
    "    \n",
    "    # ä¸Šä½3å€‹ã®ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®æ•£å¸ƒå›³\n",
    "    corr = train_df[numeric_cols + [target_col]].corr()[target_col]\n",
    "    for i, feature in enumerate(top_features[:3], 1):\n",
    "        axes[i].scatter(train_df[feature], train_df[target_col], alpha=0.3)\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel(target_col)\n",
    "        axes[i].set_title(f'{feature} (ç›¸é–¢: {corr[feature]:.2f})')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ğŸ’¡ ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡: {top_features[:3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›¸é–¢è¡Œåˆ—ã¨å¤–ã‚Œå€¤ã®ä¸€æ‹¬å¯è¦–åŒ–\n",
    "if target_col in train_df.columns:\n",
    "    # æ•°å€¤ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆã¾ã å®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰\n",
    "    if 'numeric_cols' not in locals():\n",
    "        numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if 'Id' in numeric_cols:\n",
    "            numeric_cols.remove('Id')\n",
    "        if target_col in numeric_cols:\n",
    "            numeric_cols.remove(target_col)\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "        correlations = train_df[numeric_cols + [target_col]].corr()[target_col].abs().sort_values(ascending=False)\n",
    "        top_features = correlations.head(10).index.tolist()\n",
    "        if target_col in top_features:\n",
    "            top_features.remove(target_col)\n",
    "        \n",
    "        # 2ã¤ã®å›³ã‚’ä¸¦ã¹ã¦è¡¨ç¤º\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "        \n",
    "        # 1. ç›¸é–¢è¡Œåˆ—ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼‰\n",
    "        corr_matrix = train_df[top_features[:8] + [target_col]].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                    square=True, linewidths=0.5, ax=axes[0], cbar_kws={\"shrink\": 0.8})\n",
    "        axes[0].set_title('ç‰¹å¾´é‡é–“ã®ç›¸é–¢è¡Œåˆ—ï¼ˆé‡è¦ãªç‰¹å¾´é‡ã®ã¿ï¼‰')\n",
    "        \n",
    "        # 2. ç›®çš„å¤‰æ•°ã®ç®±ã²ã’å›³ï¼ˆå¤–ã‚Œå€¤æ¤œå‡ºï¼‰\n",
    "        axes[1].boxplot(train_df[target_col])\n",
    "        axes[1].set_title(f'{target_col}ã®ç®±ã²ã’å›³ï¼ˆå¤–ã‚Œå€¤æ¤œå‡ºï¼‰')\n",
    "        axes[1].set_ylabel('ä¾¡æ ¼ï¼ˆãƒ‰ãƒ«ï¼‰')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # å¤–ã‚Œå€¤ã®æ•°ã ã‘è¡¨ç¤º\n",
    "        Q1 = train_df[target_col].quantile(0.25)\n",
    "        Q3 = train_df[target_col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers_count = len(train_df[(train_df[target_col] < Q1 - 1.5*IQR) | \n",
    "                                       (train_df[target_col] > Q3 + 1.5*IQR)])\n",
    "        print(f\"ğŸ’¡ å¤–ã‚Œå€¤ã®æ•°: {outliers_count} ({outliers_count/len(train_df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®é–¢ä¿‚ï¼ˆç°¡æ½”ç‰ˆï¼‰\n",
    "categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if target_col in train_df.columns and len(categorical_cols) > 0:\n",
    "    # ä¾¡æ ¼å·®ãŒå¤§ãã„ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’è‡ªå‹•ã§è¦‹ã¤ã‘ã‚‹\n",
    "    cat_price_ranges = {}\n",
    "    for col in categorical_cols[:5]:  # æœ€åˆã®5å€‹ã‚’åˆ†æ\n",
    "        price_range = train_df.groupby(col)[target_col].mean().max() - train_df.groupby(col)[target_col].mean().min()\n",
    "        cat_price_ranges[col] = price_range\n",
    "    \n",
    "    if len(cat_price_ranges) > 0:\n",
    "        best_cat = max(cat_price_ranges, key=cat_price_ranges.get)\n",
    "        \n",
    "        # å¯è¦–åŒ–\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        train_df.boxplot(column=target_col, by=best_cat, ax=plt.gca())\n",
    "        plt.title(f'{best_cat} ã¨ ä½å®…ä¾¡æ ¼ã®é–¢ä¿‚')\n",
    "        plt.suptitle('')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"ğŸ’¡ ä¾¡æ ¼å·®ãŒæœ€ã‚‚å¤§ãã„ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡: {best_cat}\")\n",
    "        print(f\"   ä¾¡æ ¼å·®: ${cat_price_ranges[best_cat]:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ydata-profilingã‚’ä½¿ã£ãŸè‡ªå‹•EDAï¼ˆæœ€ã‚‚ç°¡å˜ï¼‰\n",
    "\n",
    "1è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ã€åŒ…æ‹¬çš„ãªEDAãƒ¬ãƒãƒ¼ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã§ãã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ydata-profilingã§è‡ªå‹•EDAãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæœ€ã‚‚ç°¡å˜ï¼‰\n",
    "# 1è¡Œã®ã‚³ãƒ¼ãƒ‰ã§åŒ…æ‹¬çš„ãªEDAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ\n",
    "\n",
    "if EDA_AVAILABLE:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“Š è‡ªå‹•EDAãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå¤§ãã„å ´åˆã€æ•°åˆ†ã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ï¼‰\n",
    "    sample_size = min(1000, len(train_df))\n",
    "    train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    print(f\"\\nã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆ{sample_size}è¡Œï¼‰ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™...\")\n",
    "    print(\"   å…¨ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€sample_sizeã‚’å¢—ã‚„ã—ã¦ãã ã•ã„\")\n",
    "    \n",
    "    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
    "    profile = ProfileReport(\n",
    "        train_sample, \n",
    "        title=\"House Prices - EDAãƒ¬ãƒãƒ¼ãƒˆ\",\n",
    "        minimal=True  # é«˜é€ŸåŒ–ã®ãŸã‚ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰\n",
    "    )\n",
    "    \n",
    "    # HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜\n",
    "    profile.to_file(\"../eda_report_house_prices.html\")\n",
    "    print(\"âœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: ../eda_report_house_prices.html\")\n",
    "    print(\"   ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã„ã¦ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "    \n",
    "    # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã§è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "    # profile.to_notebook_iframe()\n",
    "else:\n",
    "    print(\"âš ï¸ ydata-profilingãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
    "    print(\"   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install ydata-profiling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è©³ç´°ãªEDA: æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®åˆ†æ\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚Šæ·±ãç†è§£ã™ã‚‹ãŸã‚ã®åˆ†æã‚’è¡Œã„ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é›¢\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# IDåˆ—ã¨ç›®çš„å¤‰æ•°ã‚’é™¤å¤–\n",
    "if 'Id' in numeric_cols:\n",
    "    numeric_cols.remove('Id')\n",
    "if 'SalePrice' in numeric_cols:\n",
    "    numeric_cols.remove('SalePrice')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š ç‰¹å¾´é‡ã®åˆ†é¡\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"æ•°å€¤ç‰¹å¾´é‡: {len(numeric_cols)}å€‹\")\n",
    "print(f\"ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡: {len(categorical_cols)}å€‹\")\n",
    "print(f\"åˆè¨ˆ: {len(numeric_cols) + len(categorical_cols)}å€‹\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°å€¤ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ï¼ˆSalePriceï¼‰ã®ç›¸é–¢ã‚’ç¢ºèª\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š æ•°å€¤ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®ç›¸é–¢ï¼ˆä¸Šä½20å€‹ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'SalePrice' in train_df.columns and len(numeric_cols) > 0:\n",
    "    # ç›¸é–¢ã‚’è¨ˆç®—\n",
    "    correlations = train_df[numeric_cols + ['SalePrice']].corr()['SalePrice'].sort_values(ascending=False)\n",
    "    correlations = correlations.drop('SalePrice')  # è‡ªåˆ†è‡ªèº«ã¨ã®ç›¸é–¢ã‚’é™¤å¤–\n",
    "    \n",
    "    # ä¸Šä½20å€‹ã‚’è¡¨ç¤º\n",
    "    top_correlations = correlations.head(20)\n",
    "    print(\"\\næ­£ã®ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ï¼ˆä¸Šä½10å€‹ï¼‰:\")\n",
    "    print(top_correlations.head(10))\n",
    "    \n",
    "    print(\"\\nè² ã®ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ï¼ˆä¸‹ä½10å€‹ï¼‰:\")\n",
    "    print(correlations.tail(10))\n",
    "    \n",
    "    # å¯è¦–åŒ–\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    top_correlations.plot(kind='barh')\n",
    "    plt.title('æ•°å€¤ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®ç›¸é–¢ï¼ˆä¸Šä½20å€‹ï¼‰')\n",
    "    plt.xlabel('ç›¸é–¢ä¿‚æ•°')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ç›¸é–¢ã®å¼·ã„ç‰¹å¾´é‡ã‚’ä¿å­˜ï¼ˆå¾Œã§ä½¿ã†ãŸã‚ï¼‰\n",
    "    high_corr_features = correlations[abs(correlations) > 0.3].index.tolist()\n",
    "    print(f\"\\nğŸ’¡ ç›¸é–¢ãŒå¼·ã„ç‰¹å¾´é‡ï¼ˆ|ç›¸é–¢| > 0.3ï¼‰: {len(high_corr_features)}å€‹\")\n",
    "    print(f\"   ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ã¯ä¾¡æ ¼äºˆæ¸¬ã«é‡è¦ã§ã™\")\n",
    "else:\n",
    "    print(\"âš ï¸ ç›®çš„å¤‰æ•°ã¾ãŸã¯æ•°å€¤ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®é–¢ä¿‚ã‚’ç¢ºèª\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®é–¢ä¿‚\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'SalePrice' in train_df.columns and len(categorical_cols) > 0:\n",
    "    # å„ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®çµ±è¨ˆã‚’è¨ˆç®—\n",
    "    cat_stats = []\n",
    "    \n",
    "    for col in categorical_cols[:10]:  # æœ€åˆã®10å€‹ã‚’åˆ†æ\n",
    "        stats = train_df.groupby(col)['SalePrice'].agg(['count', 'mean', 'std']).sort_values('mean')\n",
    "        stats['cv'] = stats['std'] / stats['mean']  # å¤‰å‹•ä¿‚æ•°\n",
    "        \n",
    "        cat_stats.append({\n",
    "            'ç‰¹å¾´é‡': col,\n",
    "            'ã‚«ãƒ†ã‚´ãƒªæ•°': train_df[col].nunique(),\n",
    "            'å¹³å‡ä¾¡æ ¼ã®ç¯„å›²': stats['mean'].max() - stats['mean'].min(),\n",
    "            'æœ€å°å¹³å‡ä¾¡æ ¼': stats['mean'].min(),\n",
    "            'æœ€å¤§å¹³å‡ä¾¡æ ¼': stats['mean'].max()\n",
    "        })\n",
    "    \n",
    "    cat_stats_df = pd.DataFrame(cat_stats)\n",
    "    print(\"\\nã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®çµ±è¨ˆï¼ˆæœ€åˆã®10å€‹ï¼‰:\")\n",
    "    print(cat_stats_df)\n",
    "    \n",
    "    # æœ€ã‚‚ä¾¡æ ¼å·®ãŒå¤§ãã„ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’å¯è¦–åŒ–\n",
    "    if len(cat_stats_df) > 0:\n",
    "        best_cat = cat_stats_df.loc[cat_stats_df['å¹³å‡ä¾¡æ ¼ã®ç¯„å›²'].idxmax(), 'ç‰¹å¾´é‡']\n",
    "        print(f\"\\nğŸ’¡ ä¾¡æ ¼å·®ãŒæœ€ã‚‚å¤§ãã„ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡: {best_cat}\")\n",
    "        \n",
    "        # å¯è¦–åŒ–\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        train_df.boxplot(column='SalePrice', by=best_cat, ax=plt.gca())\n",
    "        plt.title(f'{best_cat} ã¨ ä½å®…ä¾¡æ ¼ã®é–¢ä¿‚')\n",
    "        plt.suptitle('')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‰Šé™¤\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ ç›®çš„å¤‰æ•°ã¾ãŸã¯ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã‚ªãƒ—ã‚·ãƒ§ãƒ³: pandas-profilingã‚’ä½¿ã£ãŸè‡ªå‹•EDAï¼ˆæœ€ã‚‚ç°¡å˜ï¼‰\n",
    "\n",
    "1è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ã€åŒ…æ‹¬çš„ãªEDAãƒ¬ãƒãƒ¼ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã§ãã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ydata-profilingã§è‡ªå‹•EDAãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæœ€ã‚‚ç°¡å˜ï¼‰\n",
    "# 1è¡Œã®ã‚³ãƒ¼ãƒ‰ã§åŒ…æ‹¬çš„ãªEDAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ\n",
    "\n",
    "if EDA_AVAILABLE:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“Š è‡ªå‹•EDAãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå¤§ãã„å ´åˆã€æ•°åˆ†ã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n",
    "    \n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ï¼‰\n",
    "    sample_size = min(1000, len(train_df))\n",
    "    train_sample = train_df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    print(f\"\\nã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆ{sample_size}è¡Œï¼‰ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™...\")\n",
    "    print(\"   å…¨ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€sample_sizeã‚’å¢—ã‚„ã—ã¦ãã ã•ã„\")\n",
    "    \n",
    "    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
    "    profile = ProfileReport(\n",
    "        train_sample, \n",
    "        title=\"House Prices - EDAãƒ¬ãƒãƒ¼ãƒˆ\",\n",
    "        minimal=True  # é«˜é€ŸåŒ–ã®ãŸã‚ç°¡æ˜“ãƒ¢ãƒ¼ãƒ‰\n",
    "    )\n",
    "    \n",
    "    # HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜\n",
    "    profile.to_file(\"../eda_report_house_prices.html\")\n",
    "    print(\"âœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: ../eda_report_house_prices.html\")\n",
    "    print(\"   ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã„ã¦ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "    \n",
    "    # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã§è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "    # profile.to_notebook_iframe()\n",
    "else:\n",
    "    print(\"âš ï¸ ydata-profilingãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
    "    print(\"   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install ydata-profiling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰‹å‹•EDA: è©³ç´°ãªåˆ†æãŒå¿…è¦ãªå ´åˆ\n",
    "\n",
    "è‡ªå‹•EDAã§ä¸ååˆ†ãªå ´åˆã€ä»¥ä¸‹ã®ã‚»ãƒ«ã§è©³ç´°ãªåˆ†æã‚’è¡Œãˆã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©³ç´°ãªEDAï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè¡Œï¼‰\n",
    "\n",
    "# æ•°å€¤ç‰¹å¾´é‡ã¨ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚’åˆ†é›¢\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if 'Id' in numeric_cols:\n",
    "    numeric_cols.remove('Id')\n",
    "if 'SalePrice' in numeric_cols:\n",
    "    numeric_cols.remove('SalePrice')\n",
    "\n",
    "print(f\"æ•°å€¤ç‰¹å¾´é‡: {len(numeric_cols)}å€‹\")\n",
    "print(f\"ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡: {len(categorical_cols)}å€‹\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒæ¯”è¼ƒï¼ˆç°¡æ½”ç‰ˆï¼‰\n",
    "if len(numeric_cols) > 0:\n",
    "    # ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "    correlations = train_df[numeric_cols + [target_col]].corr()[target_col].abs().sort_values(ascending=False)\n",
    "    top_features = correlations.head(3).index.tolist()\n",
    "    if target_col in top_features:\n",
    "        top_features.remove(target_col)\n",
    "    \n",
    "    # å¯è¦–åŒ–\n",
    "    fig, axes = plt.subplots(1, len(top_features), figsize=(5*len(top_features), 5))\n",
    "    if len(top_features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, feature in enumerate(top_features):\n",
    "        axes[i].hist(train_df[feature].dropna(), bins=30, alpha=0.5, label='è¨“ç·´', density=True)\n",
    "        axes[i].hist(test_df[feature].dropna(), bins=30, alpha=0.5, label='ãƒ†ã‚¹ãƒˆ', density=True)\n",
    "        axes[i].set_title(feature)\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ’¡ åˆ†å¸ƒã®é•ã„ãŒå¤§ãã„å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤–ã‚Œå€¤ï¼ˆOutlierï¼‰ã®æ¤œå‡º\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š å¤–ã‚Œå€¤ã®æ¤œå‡º\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'SalePrice' in train_df.columns:\n",
    "    # ç›®çš„å¤‰æ•°ã®å¤–ã‚Œå€¤ã‚’æ¤œå‡ºï¼ˆIQRæ³•ï¼‰\n",
    "    Q1 = train_df['SalePrice'].quantile(0.25)\n",
    "    Q3 = train_df['SalePrice'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = train_df[(train_df['SalePrice'] < lower_bound) | (train_df['SalePrice'] > upper_bound)]\n",
    "    \n",
    "    print(f\"å¤–ã‚Œå€¤ã®å®šç¾©:\")\n",
    "    print(f\"  ä¸‹é™: ${lower_bound:,.2f}\")\n",
    "    print(f\"  ä¸Šé™: ${upper_bound:,.2f}\")\n",
    "    print(f\"  å¤–ã‚Œå€¤ã®æ•°: {len(outliers)} ({len(outliers)/len(train_df)*100:.2f}%)\")\n",
    "    \n",
    "    # å¯è¦–åŒ–\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # ç®±ã²ã’å›³\n",
    "    axes[0].boxplot(train_df['SalePrice'])\n",
    "    axes[0].set_title('ä½å®…ä¾¡æ ¼ã®ç®±ã²ã’å›³ï¼ˆå¤–ã‚Œå€¤æ¤œå‡ºï¼‰')\n",
    "    axes[0].set_ylabel('ä¾¡æ ¼ï¼ˆãƒ‰ãƒ«ï¼‰')\n",
    "    \n",
    "    # æ•£å¸ƒå›³ï¼ˆID vs ä¾¡æ ¼ï¼‰\n",
    "    axes[1].scatter(train_df.index, train_df['SalePrice'], alpha=0.5)\n",
    "    axes[1].axhline(y=upper_bound, color='r', linestyle='--', label=f'ä¸Šé™: ${upper_bound:,.0f}')\n",
    "    axes[1].axhline(y=lower_bound, color='r', linestyle='--', label=f'ä¸‹é™: ${lower_bound:,.0f}')\n",
    "    axes[1].set_title('ä½å®…ä¾¡æ ¼ã®æ•£å¸ƒå›³ï¼ˆå¤–ã‚Œå€¤æ¤œå‡ºï¼‰')\n",
    "    axes[1].set_xlabel('ã‚µãƒ³ãƒ—ãƒ«ç•ªå·')\n",
    "    axes[1].set_ylabel('ä¾¡æ ¼ï¼ˆãƒ‰ãƒ«ï¼‰')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if len(outliers) > 0:\n",
    "        print(f\"\\nâš ï¸ å¤–ã‚Œå€¤ã®çµ±è¨ˆæƒ…å ±:\")\n",
    "        print(outliers['SalePrice'].describe())\n",
    "else:\n",
    "    print(\"âš ï¸ ç›®çš„å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡è¦ãªæ•°å€¤ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®æ•£å¸ƒå›³\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š é‡è¦ãªæ•°å€¤ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®é–¢ä¿‚\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'SalePrice' in train_df.columns and len(numeric_cols) > 0:\n",
    "    # ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "    correlations = train_df[numeric_cols + ['SalePrice']].corr()['SalePrice'].abs().sort_values(ascending=False)\n",
    "    top_features = correlations.head(6).index.tolist()  # ä¸Šä½6å€‹\n",
    "    \n",
    "    if 'SalePrice' in top_features:\n",
    "        top_features.remove('SalePrice')\n",
    "    \n",
    "    # æ•£å¸ƒå›³ã‚’æç”»\n",
    "    n_features = min(6, len(top_features))\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(top_features[:6]):\n",
    "        axes[i].scatter(train_df[feature], train_df['SalePrice'], alpha=0.5)\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('SalePrice')\n",
    "        axes[i].set_title(f'{feature} vs SalePrice\\n(ç›¸é–¢: {correlations[feature]:.3f})')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ è¡¨ç¤ºã—ãŸç‰¹å¾´é‡: {top_features[:6]}\")\n",
    "    print(\"   ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ã¯ä¾¡æ ¼äºˆæ¸¬ã«é‡è¦ã§ã™\")\n",
    "else:\n",
    "    print(\"âš ï¸ ç›®çš„å¤‰æ•°ã¾ãŸã¯æ•°å€¤ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›¸é–¢è¡Œåˆ—ã®å¯è¦–åŒ–ï¼ˆé‡è¦ãªç‰¹å¾´é‡ã®ã¿ï¼‰\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š ç‰¹å¾´é‡é–“ã®ç›¸é–¢è¡Œåˆ—ï¼ˆé‡è¦ãªç‰¹å¾´é‡ã®ã¿ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'SalePrice' in train_df.columns and len(numeric_cols) > 0:\n",
    "    # ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "    correlations = train_df[numeric_cols + ['SalePrice']].corr()['SalePrice'].abs().sort_values(ascending=False)\n",
    "    top_features = correlations.head(15).index.tolist()  # ä¸Šä½15å€‹\n",
    "    \n",
    "    # ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—\n",
    "    corr_matrix = train_df[top_features].corr()\n",
    "    \n",
    "    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('ç‰¹å¾´é‡é–“ã®ç›¸é–¢è¡Œåˆ—ï¼ˆé‡è¦ãªç‰¹å¾´é‡ã®ã¿ï¼‰')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ ç›¸é–¢è¡Œåˆ—ã®è¦‹æ–¹:\")\n",
    "    print(\"  - 1ã«è¿‘ã„ï¼ˆèµ¤ï¼‰: å¼·ã„æ­£ã®ç›¸é–¢\")\n",
    "    print(\"  - -1ã«è¿‘ã„ï¼ˆé’ï¼‰: å¼·ã„è² ã®ç›¸é–¢\")\n",
    "    print(\"  - 0ã«è¿‘ã„ï¼ˆç™½ï¼‰: ç›¸é–¢ãŒå¼±ã„\")\n",
    "    print(\"  - ç›¸é–¢ãŒé«˜ã„ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›ã¯ã€å¤šé‡å…±ç·šæ€§ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n",
    "else:\n",
    "    print(\"âš ï¸ ç›®çš„å¤‰æ•°ã¾ãŸã¯æ•°å€¤ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã®æ¯”è¼ƒ\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒæ¯”è¼ƒ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    # é‡è¦ãªæ•°å€¤ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "    correlations = train_df[numeric_cols + ['SalePrice']].corr()['SalePrice'].abs().sort_values(ascending=False)\n",
    "    top_features = correlations.head(5).index.tolist()\n",
    "    \n",
    "    if 'SalePrice' in top_features:\n",
    "        top_features.remove('SalePrice')\n",
    "    \n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å€¤ã‚’æ¯”è¼ƒ\n",
    "    comparison = pd.DataFrame({\n",
    "        'è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡': train_df[top_features].mean(),\n",
    "        'ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡': test_df[top_features].mean()\n",
    "    })\n",
    "    comparison['å·®'] = comparison['ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å¹³å‡'] - comparison['è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡']\n",
    "    comparison['å·®ã®å‰²åˆ(%)'] = (comparison['å·®'] / comparison['è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡']) * 100\n",
    "    \n",
    "    print(\"\\né‡è¦ãªç‰¹å¾´é‡ã®å¹³å‡å€¤ã®æ¯”è¼ƒ:\")\n",
    "    print(comparison)\n",
    "    \n",
    "    # å¯è¦–åŒ–\n",
    "    fig, axes = plt.subplots(1, len(top_features), figsize=(5*len(top_features), 5))\n",
    "    if len(top_features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, feature in enumerate(top_features):\n",
    "        axes[i].hist(train_df[feature].dropna(), bins=30, alpha=0.5, label='è¨“ç·´ãƒ‡ãƒ¼ã‚¿', density=True)\n",
    "        axes[i].hist(test_df[feature].dropna(), bins=30, alpha=0.5, label='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿', density=True)\n",
    "        axes[i].set_title(f'{feature}\\nè¨“ç·´å¹³å‡: {train_df[feature].mean():.1f}\\nãƒ†ã‚¹ãƒˆå¹³å‡: {test_df[feature].mean():.1f}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('å¯†åº¦')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ åˆ†å¸ƒã®é•ã„ãŒå¤§ãã„å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ•°å€¤ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDAã®ã¾ã¨ã‚\n",
    "\n",
    "ã“ã‚Œã¾ã§ã®EDAã§åˆ†ã‹ã£ãŸã“ã¨ã‚’ã¾ã¨ã‚ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDAã®ã¾ã¨ã‚\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“‹ EDAã®ã¾ã¨ã‚\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ¬ æå€¤ã®å†è¨ˆç®—ï¼ˆå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã«å‚™ãˆã¦ï¼‰\n",
    "missing = train_df.isnull().sum()\n",
    "missing_df_summary = missing[missing > 0]\n",
    "\n",
    "summary = {\n",
    "    'ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º': f\"{train_df.shape[0]}è¡Œ Ã— {train_df.shape[1]}åˆ—\",\n",
    "    'æ•°å€¤ç‰¹å¾´é‡': f\"{len(numeric_cols)}å€‹\" if 'numeric_cols' in locals() else \"æœªè¨ˆç®—\",\n",
    "    'ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡': f\"{len(categorical_cols)}å€‹\" if 'categorical_cols' in locals() else \"æœªè¨ˆç®—\",\n",
    "    'æ¬ æå€¤ãŒã‚ã‚‹åˆ—': f\"{len(missing_df_summary)}åˆ—\"\n",
    "}\n",
    "\n",
    "if 'SalePrice' in train_df.columns:\n",
    "    summary['ç›®çš„å¤‰æ•°ã®ç¯„å›²'] = f\"${train_df['SalePrice'].min():,.0f} - ${train_df['SalePrice'].max():,.0f}\"\n",
    "    summary['ç›®çš„å¤‰æ•°ã®å¹³å‡'] = f\"${train_df['SalePrice'].mean():,.0f}\"\n",
    "    summary['ç›®çš„å¤‰æ•°ã®æ­ªåº¦'] = f\"{train_df['SalePrice'].skew():.4f}\"\n",
    "\n",
    "print(\"\\n\".join([f\"  {k}: {v}\" for k, v in summary.items()]))\n",
    "\n",
    "print(\"\\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "print(\"  1. æ¬ æå€¤ã®å‡¦ç†\")\n",
    "print(\"  2. å¤–ã‚Œå€¤ã®å‡¦ç†ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\")\n",
    "print(\"  3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\")\n",
    "print(\"  4. ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\")\n",
    "print(\"  5. ç›®çš„å¤‰æ•°ã®å¤‰æ›ï¼ˆå¯¾æ•°å¤‰æ›ãªã©ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ—4: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆé‡è¦ï¼ï¼‰\n",
    "\n",
    "House Pricesã‚³ãƒ³ãƒšã§ã¯ã€**ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**ãŒã‚¹ã‚³ã‚¢å‘ä¸Šã®éµã§ã™ã€‚\n",
    "\n",
    "### ã‚ˆãä½¿ã‚ã‚Œã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "\n",
    "1. **æ•°å€¤ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›**\n",
    "   - ç·é¢ç© = 1éšã®é¢ç© + 2éšã®é¢ç©\n",
    "   - éƒ¨å±‹æ•°æ¯”ç‡ = éƒ¨å±‹æ•° / ç·é¢ç©\n",
    "\n",
    "2. **ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®å‡¦ç†**\n",
    "   - Label Encoding\n",
    "   - One-Hot Encoding\n",
    "\n",
    "3. **ç›®çš„å¤‰æ•°ã®å¤‰æ›**\n",
    "   - å¯¾æ•°å¤‰æ›ï¼ˆä¾¡æ ¼ãŒæ­ªã‚“ã§ã„ã‚‹å ´åˆï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰å‡¦ç†ã®æº–å‚™\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦ã€åŒã˜å‰å‡¦ç†ã‚’é©ç”¨\n",
    "\n",
    "# IDåˆ—ã‚’ä¿å­˜\n",
    "train_ids = train_df['Id'].copy() if 'Id' in train_df.columns else train_df.index\n",
    "test_ids = test_df['Id'].copy() if 'Id' in test_df.columns else test_df.index\n",
    "\n",
    "# ç›®çš„å¤‰æ•°ã‚’ä¿å­˜\n",
    "y_train = train_df[target_col].copy() if target_col in train_df.columns else None\n",
    "\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆå‰å‡¦ç†ã‚’çµ±ä¸€ã™ã‚‹ãŸã‚ï¼‰\n",
    "if target_col in train_df.columns:\n",
    "    test_df_temp = test_df.copy()\n",
    "    test_df_temp[target_col] = -1  # ãƒ€ãƒŸãƒ¼å€¤\n",
    "    combined_df = pd.concat([train_df, test_df_temp], axis=0, ignore_index=True)\n",
    "else:\n",
    "    combined_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {combined_df.shape}\")\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_df)}è¡Œ\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df)}è¡Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ä¾‹1: ç·é¢ç©ã®ä½œæˆï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "if 'TotalBsmtSF' in combined_df.columns and 'GrLivArea' in combined_df.columns:\n",
    "    combined_df['TotalSF'] = combined_df['TotalBsmtSF'] + combined_df['GrLivArea']\n",
    "    print(\"âœ… ç·é¢ç©ï¼ˆTotalSFï¼‰ã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "\n",
    "# ä¾‹2: éƒ¨å±‹æ•°ã¨é¢ç©ã®æ¯”ç‡ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "if 'TotRmsAbvGrd' in combined_df.columns and 'GrLivArea' in combined_df.columns:\n",
    "    combined_df['RoomsPerArea'] = combined_df['TotRmsAbvGrd'] / (combined_df['GrLivArea'] + 1)\n",
    "    print(\"âœ… éƒ¨å±‹æ•°æ¯”ç‡ï¼ˆRoomsPerAreaï¼‰ã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "\n",
    "# ä¾‹3: ç¯‰å¹´æ•°ã¨ãƒªãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å¹´ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "if 'YearBuilt' in combined_df.columns and 'YearRemodAdd' in combined_df.columns:\n",
    "    combined_df['Age'] = 2024 - combined_df['YearBuilt']\n",
    "    combined_df['RemodAge'] = 2024 - combined_df['YearRemodAdd']\n",
    "    print(\"âœ… ç¯‰å¹´æ•°ï¼ˆAgeï¼‰ã¨ãƒªãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å¹´ï¼ˆRemodAgeï¼‰ã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "\n",
    "print(\"\\nâœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¬ æå€¤ã®å‡¦ç†\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”§ æ¬ æå€¤ã®å‡¦ç†\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ•°å€¤åˆ—ã®æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§åŸ‹ã‚ã‚‹\n",
    "numeric_cols = combined_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Id' in numeric_cols:\n",
    "    numeric_cols.remove('Id')\n",
    "if target_col in numeric_cols:\n",
    "    numeric_cols.remove(target_col)\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    missing_numeric = [col for col in numeric_cols if combined_df[col].isnull().sum() > 0]\n",
    "    if len(missing_numeric) > 0:\n",
    "        print(f\"æ•°å€¤åˆ—ã®æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§åŸ‹ã‚ã¾ã™: {len(missing_numeric)}åˆ—\")\n",
    "        imputer_numeric = SimpleImputer(strategy='median')\n",
    "        combined_df[numeric_cols] = imputer_numeric.fit_transform(combined_df[numeric_cols])\n",
    "        print(\"âœ… æ•°å€¤åˆ—ã®æ¬ æå€¤å‡¦ç†å®Œäº†\")\n",
    "    else:\n",
    "        print(\"âœ… æ•°å€¤åˆ—ã«æ¬ æå€¤ã¯ã‚ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤ã‚’æœ€é »å€¤ã§åŸ‹ã‚ã‚‹\n",
    "categorical_cols = combined_df.select_dtypes(include=['object']).columns.tolist()\n",
    "if len(categorical_cols) > 0:\n",
    "    missing_categorical = [col for col in categorical_cols if combined_df[col].isnull().sum() > 0]\n",
    "    if len(missing_categorical) > 0:\n",
    "        print(f\"ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤ã‚’æœ€é »å€¤ã§åŸ‹ã‚ã¾ã™: {len(missing_categorical)}åˆ—\")\n",
    "        imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "        combined_df[categorical_cols] = imputer_categorical.fit_transform(combined_df[categorical_cols])\n",
    "        print(\"âœ… ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ¬ æå€¤å‡¦ç†å®Œäº†\")\n",
    "    else:\n",
    "        print(\"âœ… ã‚«ãƒ†ã‚´ãƒªåˆ—ã«æ¬ æå€¤ã¯ã‚ã‚Šã¾ã›ã‚“\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã®æ•°å€¤åŒ–ï¼ˆLabel Encodingï¼‰\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”§ ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã®æ•°å€¤åŒ–\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "categorical_cols_to_encode = combined_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if len(categorical_cols_to_encode) > 0:\n",
    "    print(f\"ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’æ•°å€¤åŒ–ã—ã¾ã™: {len(categorical_cols_to_encode)}åˆ—\")\n",
    "    for col in categorical_cols_to_encode:\n",
    "        le = LabelEncoder()\n",
    "        combined_df[col] = le.fit_transform(combined_df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    print(\"âœ… ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã®æ•°å€¤åŒ–å®Œäº†\")\n",
    "else:\n",
    "    print(\"âœ… ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å†åº¦åˆ†é›¢\n",
    "train_processed = combined_df[:len(train_df)].copy()\n",
    "test_processed = combined_df[len(train_df):].copy()\n",
    "\n",
    "# ç›®çš„å¤‰æ•°ã‚’å¾©å…ƒ\n",
    "if y_train is not None:\n",
    "    train_processed[target_col] = y_train\n",
    "\n",
    "# IDåˆ—ã¨ç›®çš„å¤‰æ•°ã‚’é™¤å¤–ã—ã¦ç‰¹å¾´é‡ã®ã¿ã«ã™ã‚‹\n",
    "exclude_cols = ['Id']\n",
    "if target_col in train_processed.columns:\n",
    "    exclude_cols.append(target_col)\n",
    "\n",
    "feature_cols = [col for col in train_processed.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = train_processed[feature_cols].copy()\n",
    "y_train_final = train_processed[target_col].copy() if target_col in train_processed.columns else None\n",
    "X_test = test_processed[feature_cols].copy()\n",
    "\n",
    "print(f\"âœ… å‰å‡¦ç†å®Œäº†\")\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡æ•°: {X_train.shape[1]}\")\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {X_train.shape[0]}\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {X_test.shape[0]}\")\n",
    "\n",
    "# ç›®çš„å¤‰æ•°ã®å¯¾æ•°å¤‰æ›ï¼ˆæ­ªåº¦ãŒå¤§ãã„å ´åˆï¼‰\n",
    "if y_train_final is not None:\n",
    "    if y_train_final.skew() > 0.5:  # æ­ªåº¦ãŒ0.5ã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "        print(f\"\\nğŸ“Š ç›®çš„å¤‰æ•°ã®å¯¾æ•°å¤‰æ›ã‚’é©ç”¨ã—ã¾ã™ï¼ˆæ­ªåº¦: {y_train_final.skew():.4f}ï¼‰\")\n",
    "        y_train_final_log = np.log1p(y_train_final)  # log1p = log(1+x)ã§0ã‚’æ‰±ã„ã‚„ã™ã„\n",
    "        use_log_transform = True\n",
    "    else:\n",
    "        y_train_final_log = y_train_final.copy()\n",
    "        use_log_transform = False\n",
    "        print(f\"\\nğŸ“Š ç›®çš„å¤‰æ•°ã®å¯¾æ•°å¤‰æ›ã¯ä¸è¦ã§ã™ï¼ˆæ­ªåº¦: {y_train_final.skew():.4f}ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
    "\n",
    "å›å¸°å•é¡Œã§ã¯ã€ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆãä½¿ã‚ã‚Œã¾ã™ï¼š\n",
    "\n",
    "1. **ç·šå½¢å›å¸°ï¼ˆLinear Regressionï¼‰**\n",
    "   - ã‚·ãƒ³ãƒ—ãƒ«ã§è§£é‡ˆã—ã‚„ã™ã„\n",
    "   - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹\n",
    "\n",
    "2. **Ridge / Lassoå›å¸°**\n",
    "   - æ­£å‰‡åŒ–ã‚’åŠ ãˆãŸç·šå½¢å›å¸°\n",
    "   - éå­¦ç¿’ã‚’é˜²ã\n",
    "\n",
    "3. **ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ï¼ˆRandom Forest Regressorï¼‰**\n",
    "   - éç·šå½¢é–¢ä¿‚ã‚‚å­¦ç¿’ã§ãã‚‹\n",
    "   - ç‰¹å¾´é‡ã®é‡è¦åº¦ãŒåˆ†ã‹ã‚‹\n",
    "\n",
    "4. **å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°å›å¸°ï¼ˆGradient Boosting Regressorï¼‰**\n",
    "   - é«˜ã„ç²¾åº¦ãŒå‡ºã‚„ã™ã„\n",
    "   - XGBoostã€LightGBMãªã©ãŒæœ‰å\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«1: ç·šå½¢å›å¸°ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¤– ãƒ¢ãƒ‡ãƒ«1: ç·šå½¢å›å¸°\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if y_train_final is not None:\n",
    "    # ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # å¯¾æ•°å¤‰æ›ã—ãŸç›®çš„å¤‰æ•°ã‚’ä½¿ã†\n",
    "    y_train_model = y_train_final_log if use_log_transform else y_train_final\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_scaled, y_train_model)\n",
    "    \n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬\n",
    "    y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
    "    \n",
    "    # å¯¾æ•°å¤‰æ›ã‚’å…ƒã«æˆ»ã™\n",
    "    if use_log_transform:\n",
    "        y_train_pred_lr = np.expm1(y_train_pred_lr)  # expm1 = exp(x) - 1\n",
    "    \n",
    "    # RMSEã‚’è¨ˆç®—\n",
    "    train_rmse_lr = np.sqrt(mean_squared_error(y_train_final, y_train_pred_lr))\n",
    "    train_mae_lr = mean_absolute_error(y_train_final, y_train_pred_lr)\n",
    "    \n",
    "    print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®RMSE: ${train_rmse_lr:,.2f}\")\n",
    "    print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®MAE: ${train_mae_lr:,.2f}\")\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬\n",
    "    y_test_pred_lr = lr_model.predict(X_test_scaled)\n",
    "    if use_log_transform:\n",
    "        y_test_pred_lr = np.expm1(y_test_pred_lr)\n",
    "    \n",
    "    print(\"âœ… ç·šå½¢å›å¸°ã®è¨“ç·´å®Œäº†\")\n",
    "else:\n",
    "    print(\"âš ï¸ ç›®çš„å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«2: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¤– ãƒ¢ãƒ‡ãƒ«2: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if y_train_final is not None:\n",
    "    y_train_model = y_train_final_log if use_log_transform else y_train_final\n",
    "    \n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train_model)\n",
    "    \n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬\n",
    "    y_train_pred_rf = rf_model.predict(X_train)\n",
    "    if use_log_transform:\n",
    "        y_train_pred_rf = np.expm1(y_train_pred_rf)\n",
    "    \n",
    "    train_rmse_rf = np.sqrt(mean_squared_error(y_train_final, y_train_pred_rf))\n",
    "    train_mae_rf = mean_absolute_error(y_train_final, y_train_pred_rf)\n",
    "    \n",
    "    print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®RMSE: ${train_rmse_rf:,.2f}\")\n",
    "    print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®MAE: ${train_mae_rf:,.2f}\")\n",
    "    \n",
    "    # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’è¡¨ç¤ºï¼ˆä¸Šä½10å€‹ï¼‰\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'ç‰¹å¾´é‡': X_train.columns,\n",
    "        'é‡è¦åº¦': rf_model.feature_importances_\n",
    "    }).sort_values('é‡è¦åº¦', ascending=False)\n",
    "    \n",
    "    print(\"\\nğŸ“Š ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆä¸Šä½10å€‹ï¼‰:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬\n",
    "    y_test_pred_rf = rf_model.predict(X_test)\n",
    "    if use_log_transform:\n",
    "        y_test_pred_rf = np.expm1(y_test_pred_rf)\n",
    "    \n",
    "    print(\"\\nâœ… ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ã®è¨“ç·´å®Œäº†\")\n",
    "else:\n",
    "    print(\"âš ï¸ ç›®çš„å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«3: å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°å›å¸°\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¤– ãƒ¢ãƒ‡ãƒ«3: å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°å›å¸°\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if y_train_final is not None:\n",
    "    y_train_model = y_train_final_log if use_log_transform else y_train_final\n",
    "    \n",
    "    gb_model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    gb_model.fit(X_train, y_train_model)\n",
    "    \n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬\n",
    "    y_train_pred_gb = gb_model.predict(X_train)\n",
    "    if use_log_transform:\n",
    "        y_train_pred_gb = np.expm1(y_train_pred_gb)\n",
    "    \n",
    "    train_rmse_gb = np.sqrt(mean_squared_error(y_train_final, y_train_pred_gb))\n",
    "    train_mae_gb = mean_absolute_error(y_train_final, y_train_pred_gb)\n",
    "    \n",
    "    print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®RMSE: ${train_rmse_gb:,.2f}\")\n",
    "    print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®MAE: ${train_mae_gb:,.2f}\")\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬\n",
    "    y_test_pred_gb = gb_model.predict(X_test)\n",
    "    if use_log_transform:\n",
    "        y_test_pred_gb = np.expm1(y_test_pred_gb)\n",
    "    \n",
    "    print(\"âœ… å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°å›å¸°ã®è¨“ç·´å®Œäº†\")\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®RMSEï¼‰\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        print(f\"ç·šå½¢å›å¸°: ${train_rmse_lr:,.2f}\")\n",
    "    except NameError:\n",
    "        print(\"ç·šå½¢å›å¸°: æœªå®Ÿè¡Œ\")\n",
    "    try:\n",
    "        print(f\"ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ: ${train_rmse_rf:,.2f}\")\n",
    "    except NameError:\n",
    "        print(\"ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ: æœªå®Ÿè¡Œ\")\n",
    "    print(f\"å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°: ${train_rmse_gb:,.2f}\")\n",
    "else:\n",
    "    print(\"âš ï¸ ç›®çš„å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ—6: äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "\n",
    "å›å¸°å•é¡Œã§ã¯ã€äºˆæ¸¬å€¤ãŒè² ã®å€¤ã«ãªã‚‰ãªã„ã‚ˆã†ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚\n",
    "ï¼ˆä½å®…ä¾¡æ ¼ãŒè² ã®å€¤ã«ãªã‚‹ã“ã¨ã¯ãªã„ãŸã‚ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æœ€ã‚‚è‰¯ã„æ€§èƒ½ã‚’ç¤ºã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†\n",
    "try:\n",
    "    best_predictions = y_test_pred_gb.copy()\n",
    "    print(\"âœ… å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°å›å¸°ã®äºˆæ¸¬ã‚’ä½¿ç”¨\")\n",
    "except NameError:\n",
    "    try:\n",
    "        best_predictions = y_test_pred_rf.copy()\n",
    "        print(\"âœ… ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ã®äºˆæ¸¬ã‚’ä½¿ç”¨\")\n",
    "    except NameError:\n",
    "        try:\n",
    "            best_predictions = y_test_pred_lr.copy()\n",
    "            print(\"âœ… ç·šå½¢å›å¸°ã®äºˆæ¸¬ã‚’ä½¿ç”¨\")\n",
    "        except NameError:\n",
    "            print(\"âš ï¸ ã‚¨ãƒ©ãƒ¼: äºˆæ¸¬çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "            best_predictions = None\n",
    "\n",
    "if best_predictions is not None:\n",
    "    # è² ã®å€¤ã‚’0ã«å¤‰æ›ï¼ˆä½å®…ä¾¡æ ¼ãŒè² ã«ãªã‚‹ã“ã¨ã¯ãªã„ï¼‰\n",
    "    best_predictions = np.maximum(best_predictions, 0)\n",
    "    \n",
    "    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n",
    "    submission = pd.DataFrame({\n",
    "        'Id': test_ids,\n",
    "        'SalePrice': best_predictions\n",
    "    })\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜\n",
    "    submission_file = '../submission_house_prices.csv'\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    \n",
    "    print(f\"âœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {submission_file}\")\n",
    "    print(f\"\\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®10è¡Œ:\")\n",
    "    print(submission.head(10))\n",
    "    print(f\"\\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±è¨ˆæƒ…å ±:\")\n",
    "    print(submission['SalePrice'].describe())\n",
    "else:\n",
    "    print(\"âš ï¸ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆæ”¹å–„ã®ãƒ’ãƒ³ãƒˆï¼‰\n",
    "\n",
    "### 1. ã‚ˆã‚Šé«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "- ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®çµ„ã¿åˆã‚ã›\n",
    "- ãƒãƒªãƒãƒŸã‚¢ãƒ«ç‰¹å¾´é‡ï¼ˆç‰¹å¾´é‡ã®2ä¹—ã€3ä¹—ãªã©ï¼‰\n",
    "- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "\n",
    "### 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–\n",
    "- ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã‚„ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ\n",
    "\n",
    "### 3. ã‚ˆã‚Šé«˜åº¦ãªãƒ¢ãƒ‡ãƒ«\n",
    "- XGBoostã€LightGBMã€CatBoost\n",
    "- ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ï¼‰\n",
    "\n",
    "### 4. äº¤å·®æ¤œè¨¼ã®æ´»ç”¨\n",
    "- K-Foldäº¤å·®æ¤œè¨¼ã§ã‚ˆã‚Šæ­£ç¢ºãªæ€§èƒ½è©•ä¾¡\n",
    "- éå­¦ç¿’ã‚’é˜²ã\n",
    "\n",
    "## ğŸ‰ ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§å›å¸°å•é¡Œã®åŸºæœ¬çš„ãªæµã‚Œã‚’å­¦ã³ã¾ã—ãŸã€‚\n",
    "æ¬¡ã¯ã€ä¸Šè¨˜ã®æ”¹å–„æ–¹æ³•ã‚’è©¦ã—ã¦ã€ã‚¹ã‚³ã‚¢ã‚’å‘ä¸Šã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ï¼\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
