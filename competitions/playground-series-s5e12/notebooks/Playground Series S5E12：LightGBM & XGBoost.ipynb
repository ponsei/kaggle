{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "31f8c2af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Library Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Data Setup\n",
        "train = pd.read_csv('/workspace/competitions/playground-series-s5e12/data/input/train.csv')\n",
        "test = pd.read_csv('/workspace/competitions/playground-series-s5e12/data/input/test.csv')\n",
        "\n",
        "# X,y Setup\n",
        "X = train.drop(\"diagnosed_diabetes\", axis=1)\n",
        "y = train[\"diagnosed_diabetes\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f000f58d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Feature Engineering based on Chris Deotte's 1st Place Solution ---\n",
        "\n",
        "def create_groupby_features(train, test, cat_cols, num_cols):\n",
        "    \"\"\"\n",
        "    Create groupby aggregation features: groupby(COL1)[COL2].agg(STAT)\n",
        "    \"\"\"\n",
        "    print(\"Creating groupby aggregation features...\")\n",
        "    \n",
        "    # Combine train and test for groupby calculations\n",
        "    train['is_train'] = 1\n",
        "    test['is_train'] = 0\n",
        "    df = pd.concat([train, test], ignore_index=True)\n",
        "    \n",
        "    stats = ['mean', 'std', 'count', 'min', 'max', 'nunique']\n",
        "    \n",
        "    for cat_col in cat_cols:\n",
        "        for num_col in num_cols:\n",
        "            for stat in stats:\n",
        "                try:\n",
        "                    grouped = df.groupby(cat_col)[num_col].agg(stat)\n",
        "                    new_col = f'{cat_col}_{num_col}_{stat}'\n",
        "                    train[new_col] = train[cat_col].map(grouped)\n",
        "                    test[new_col] = test[cat_col].map(grouped)\n",
        "                except:\n",
        "                    pass\n",
        "    \n",
        "    # Remove temporary column\n",
        "    train = train.drop('is_train', axis=1)\n",
        "    test = test.drop('is_train', axis=1)\n",
        "    \n",
        "    return train, test\n",
        "\n",
        "\n",
        "def create_histogram_binning_features(train, test, cat_cols, target_col='diagnosed_diabetes', n_bins=10):\n",
        "    \"\"\"\n",
        "    Create histogram binning features: groupby(COL1)[target].agg(HISTOGRAM BINS)\n",
        "    \"\"\"\n",
        "    print(\"Creating histogram binning features...\")\n",
        "    \n",
        "    # Combine train and test\n",
        "    train['is_train'] = 1\n",
        "    test['is_train'] = 0\n",
        "    df = pd.concat([train, test], ignore_index=True)\n",
        "    \n",
        "    for cat_col in cat_cols:\n",
        "        try:\n",
        "            # Get target values for each category\n",
        "            grouped = df.groupby(cat_col)[target_col]\n",
        "            \n",
        "            # Create bins for each group\n",
        "            for bin_idx in range(n_bins):\n",
        "                def get_bin_count(group):\n",
        "                    if len(group) == 0:\n",
        "                        return 0\n",
        "                    hist, _ = np.histogram(group, bins=n_bins, range=(0, 1))\n",
        "                    return hist[bin_idx] if bin_idx < len(hist) else 0\n",
        "                \n",
        "                bin_counts = grouped.agg(get_bin_count)\n",
        "                new_col = f'{cat_col}_{target_col}_hist_bin_{bin_idx}'\n",
        "                train[new_col] = train[cat_col].map(bin_counts).fillna(0)\n",
        "                test[new_col] = test[cat_col].map(bin_counts).fillna(0)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    train = train.drop('is_train', axis=1)\n",
        "    test = test.drop('is_train', axis=1)\n",
        "    \n",
        "    return train, test\n",
        "\n",
        "\n",
        "def create_quantile_features(train, test, cat_cols, num_cols, quantiles=[5, 10, 40, 45, 55, 60, 90, 95]):\n",
        "    \"\"\"\n",
        "    Create quantile features: groupby(COL1)[COL2].agg(QUANTILES)\n",
        "    \"\"\"\n",
        "    print(\"Creating quantile features...\")\n",
        "    \n",
        "    train['is_train'] = 1\n",
        "    test['is_train'] = 0\n",
        "    df = pd.concat([train, test], ignore_index=True)\n",
        "    \n",
        "    for cat_col in cat_cols:\n",
        "        for num_col in num_cols:\n",
        "            try:\n",
        "                grouped = df.groupby(cat_col)[num_col]\n",
        "                for q in quantiles:\n",
        "                    quantile_vals = grouped.quantile(q / 100.0)\n",
        "                    new_col = f'{cat_col}_{num_col}_q{q}'\n",
        "                    train[new_col] = train[cat_col].map(quantile_vals).fillna(0)\n",
        "                    test[new_col] = test[cat_col].map(quantile_vals).fillna(0)\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    train = train.drop('is_train', axis=1)\n",
        "    test = test.drop('is_train', axis=1)\n",
        "    \n",
        "    return train, test\n",
        "\n",
        "\n",
        "def create_nan_combination_features(train, test, cols_with_nan):\n",
        "    \"\"\"\n",
        "    Create a single base-2 column from all NANs across multiple columns\n",
        "    \"\"\"\n",
        "    print(\"Creating NAN combination features...\")\n",
        "    \n",
        "    # Create binary representation of NAN pattern\n",
        "    train_nan = train[cols_with_nan].isna().astype(int)\n",
        "    test_nan = test[cols_with_nan].isna().astype(int)\n",
        "    \n",
        "    # Convert to base-2 number\n",
        "    train['nan_pattern'] = 0\n",
        "    test['nan_pattern'] = 0\n",
        "    \n",
        "    for i, col in enumerate(cols_with_nan):\n",
        "        train['nan_pattern'] += train_nan[col] * (2 ** i)\n",
        "        test['nan_pattern'] += test_nan[col] * (2 ** i)\n",
        "    \n",
        "    return train, test\n",
        "\n",
        "\n",
        "def create_binning_features(train, test, num_cols, bins=[1, 2, 5, 10, 20, 50, 100]):\n",
        "    \"\"\"\n",
        "    Put numerical columns into bins using rounding\n",
        "    \"\"\"\n",
        "    print(\"Creating binning features...\")\n",
        "    \n",
        "    for num_col in num_cols:\n",
        "        for bin_size in bins:\n",
        "            try:\n",
        "                new_col = f'{num_col}_bin_{bin_size}'\n",
        "                train[new_col] = (train[num_col] / bin_size).round() * bin_size\n",
        "                test[new_col] = (test[num_col] / bin_size).round() * bin_size\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    return train, test\n",
        "\n",
        "\n",
        "def create_digit_extraction_features(train, test, num_cols, n_digits=5):\n",
        "    \"\"\"\n",
        "    Extract digits from float32 numbers\n",
        "    \"\"\"\n",
        "    print(\"Creating digit extraction features...\")\n",
        "    \n",
        "    for num_col in num_cols:\n",
        "        for digit_pos in range(1, n_digits + 1):\n",
        "            try:\n",
        "                # Extract digit at position\n",
        "                new_col = f'{num_col}_digit_{digit_pos}'\n",
        "                train[new_col] = (train[num_col] * (10 ** digit_pos)).astype(int) % 10\n",
        "                test[new_col] = (test[num_col] * (10 ** digit_pos)).astype(int) % 10\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    return train, test\n",
        "\n",
        "\n",
        "def create_categorical_combinations(train, test, cat_cols, max_combinations=28):\n",
        "    \"\"\"\n",
        "    Create combinations of categorical columns\n",
        "    \"\"\"\n",
        "    print(\"Creating categorical combinations...\")\n",
        "    \n",
        "    from itertools import combinations\n",
        "    \n",
        "    comb_count = 0\n",
        "    for r in range(2, min(len(cat_cols) + 1, 4)):  # 2 or 3 column combinations\n",
        "        for combo in combinations(cat_cols, r):\n",
        "            if comb_count >= max_combinations:\n",
        "                break\n",
        "            try:\n",
        "                new_col = '_'.join(combo) + '_combined'\n",
        "                train[new_col] = train[list(combo)].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
        "                test[new_col] = test[list(combo)].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
        "                comb_count += 1\n",
        "            except:\n",
        "                pass\n",
        "        if comb_count >= max_combinations:\n",
        "            break\n",
        "    \n",
        "    return train, test\n",
        "\n",
        "\n",
        "def create_division_features(train, test, feature_cols):\n",
        "    \"\"\"\n",
        "    Create division features from existing engineered features\n",
        "    \"\"\"\n",
        "    print(\"Creating division features...\")\n",
        "    \n",
        "    # Select some important features for division\n",
        "    if len(feature_cols) >= 2:\n",
        "        for i, col1 in enumerate(feature_cols[:10]):  # Limit to avoid too many features\n",
        "            for col2 in feature_cols[i+1:min(i+6, len(feature_cols))]:\n",
        "                try:\n",
        "                    new_col = f'{col1}_div_{col2}'\n",
        "                    train[new_col] = train[col1] / (train[col2] + 1e-8)\n",
        "                    test[new_col] = test[col1] / (test[col2] + 1e-8)\n",
        "                except:\n",
        "                    pass\n",
        "    \n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "680c5733",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Feature Engineering (Chris Deotte Style) ---\n",
        "\n",
        "# Identify column types\n",
        "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"Original features: {len(X.columns)}\")\n",
        "print(f\"Categorical columns: {len(cat_cols)}\")\n",
        "print(f\"Numerical columns: {len(num_cols)}\")\n",
        "\n",
        "# Prepare dataframes for feature engineering\n",
        "train_fe = train.copy()\n",
        "test_fe = test.copy()\n",
        "\n",
        "# 1. Basic categorical encoding\n",
        "for col in cat_cols:\n",
        "    train_fe[col] = train_fe[col].astype(\"category\")\n",
        "    test_fe[col] = test_fe[col].astype(\"category\")\n",
        "    X[col] = X[col].astype(\"category\")\n",
        "    test[col] = test[col].astype(\"category\")\n",
        "\n",
        "# 2. Groupby aggregation features (limited to avoid too many features)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Step 1: Groupby Aggregations\")\n",
        "print(\"=\"*70)\n",
        "train_fe, test_fe = create_groupby_features(train_fe, test_fe, cat_cols[:5], num_cols[:5])  # Limit columns\n",
        "\n",
        "# 3. Quantile features\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Step 2: Quantile Features\")\n",
        "print(\"=\"*70)\n",
        "train_fe, test_fe = create_quantile_features(train_fe, test_fe, cat_cols[:3], num_cols[:3])\n",
        "\n",
        "# 4. Binning features\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Step 3: Binning Features\")\n",
        "print(\"=\"*70)\n",
        "train_fe, test_fe = create_binning_features(train_fe, test_fe, num_cols[:5], bins=[1, 2, 5, 10])\n",
        "\n",
        "# 5. Digit extraction (for most important numerical column)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Step 4: Digit Extraction\")\n",
        "print(\"=\"*70)\n",
        "if len(num_cols) > 0:\n",
        "    train_fe, test_fe = create_digit_extraction_features(train_fe, test_fe, num_cols[:2], n_digits=3)\n",
        "\n",
        "# 6. Categorical combinations\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Step 5: Categorical Combinations\")\n",
        "print(\"=\"*70)\n",
        "train_fe, test_fe = create_categorical_combinations(train_fe, test_fe, cat_cols, max_combinations=10)\n",
        "\n",
        "# 7. NAN combination features\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Step 6: NAN Combination Features\")\n",
        "print(\"=\"*70)\n",
        "cols_with_nan = [col for col in train_fe.columns if train_fe[col].isna().any()]\n",
        "if len(cols_with_nan) > 0:\n",
        "    train_fe, test_fe = create_nan_combination_features(train_fe, test_fe, cols_with_nan[:10])\n",
        "\n",
        "# Update X and test with new features\n",
        "X = train_fe.drop(\"diagnosed_diabetes\", axis=1)\n",
        "test = test_fe.copy()\n",
        "\n",
        "# Handle new categorical columns\n",
        "new_cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
        "for col in new_cat_cols:\n",
        "    if col not in cat_cols:  # New categorical columns from combinations\n",
        "        X[col] = X[col].astype(\"category\")\n",
        "        test[col] = test[col].astype(\"category\")\n",
        "\n",
        "# Fill NaN values\n",
        "X = X.fillna(0)\n",
        "test = test.fillna(0)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Feature Engineering Complete!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Final features: {len(X.columns)}\")\n",
        "print(f\"New features created: {len(X.columns) - len(num_cols) - len(cat_cols)}\")\n",
        "print(\"=\"*70 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "74db6ec1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CV Setup \n",
        "FOLDS = 10\n",
        "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f0edcc3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Setup with Progress Tracking and AUC Monitoring\n",
        "def train_model(model_type, X, y, test_data, params, cat_features=None):\n",
        "    oof_preds = np.zeros(len(X))\n",
        "    test_preds = np.zeros(len(test_data))\n",
        "    models = []\n",
        "    \n",
        "    # Track AUC for train and validation\n",
        "    train_aucs = []\n",
        "    val_aucs = []\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(f\"--- Training {model_type.upper()} ---\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    fold_times = []\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
        "        fold_start = time.time()\n",
        "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Fold {fold}/{FOLDS} - Starting...\")\n",
        "        \n",
        "        X_tr, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
        "        y_tr, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
        "        \n",
        "        if model_type == 'lgb':\n",
        "            # LGBM handles categories natively if dtype is 'category'\n",
        "            model = lgb.LGBMClassifier(**params)\n",
        "            # Reduced stopping_rounds from 100 to 50 for faster training\n",
        "            callbacks = [lgb.early_stopping(stopping_rounds=50, verbose=False), lgb.log_evaluation(0)]\n",
        "            model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)], callbacks=callbacks)\n",
        "            \n",
        "        elif model_type == 'xgb':\n",
        "            # XGBoost needs enable_categorical=True for category dtypes\n",
        "            model = xgb.XGBClassifier(**params, enable_categorical=True)\n",
        "            model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)], verbose=False)\n",
        "        \n",
        "        # Predictions for both train and validation\n",
        "        train_preds = model.predict_proba(X_tr)[:, 1]\n",
        "        val_preds = model.predict_proba(X_val)[:, 1]\n",
        "        \n",
        "        oof_preds[val_idx] = val_preds\n",
        "        test_preds += model.predict_proba(test_data)[:, 1] / FOLDS\n",
        "        models.append(model)\n",
        "        \n",
        "        # Calculate AUC for train and validation\n",
        "        train_auc = roc_auc_score(y_tr, train_preds)\n",
        "        val_auc = roc_auc_score(y_val, val_preds)\n",
        "        train_aucs.append(train_auc)\n",
        "        val_aucs.append(val_auc)\n",
        "        \n",
        "        # Progress tracking\n",
        "        fold_time = time.time() - fold_start\n",
        "        fold_times.append(fold_time)\n",
        "        avg_time = np.mean(fold_times)\n",
        "        remaining_folds = FOLDS - fold\n",
        "        estimated_remaining = avg_time * remaining_folds\n",
        "        \n",
        "        elapsed_total = time.time() - start_time\n",
        "        \n",
        "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Fold {fold}/{FOLDS} - Completed!\")\n",
        "        print(f\"  Fold Time: {fold_time:.1f}s\")\n",
        "        print(f\"  Train AUC: {train_auc:.5f} | Val AUC: {val_auc:.5f} | Gap: {train_auc - val_auc:.5f}\")\n",
        "        print(f\"  Progress: {fold}/{FOLDS} ({fold/FOLDS*100:.1f}%)\")\n",
        "        if remaining_folds > 0:\n",
        "            print(f\"  Estimated Remaining: {estimated_remaining/60:.1f} minutes\")\n",
        "        print(f\"  Total Elapsed: {elapsed_total/60:.1f} minutes\")\n",
        "        \n",
        "    score = roc_auc_score(y, oof_preds)\n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"{model_type.upper()} Training Complete!\")\n",
        "    print(f\"OOF AUC: {score:.5f}\")\n",
        "    print(f\"Total Time: {total_time/60:.1f} minutes ({total_time:.1f} seconds)\")\n",
        "    print(f\"Average Time per Fold: {np.mean(fold_times):.1f} seconds\")\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"AUC Summary:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Fold':<6} {'Train AUC':<12} {'Val AUC':<12} {'Gap':<12}\")\n",
        "    print(\"-\" * 70)\n",
        "    for i in range(FOLDS):\n",
        "        gap = train_aucs[i] - val_aucs[i]\n",
        "        print(f\"{i+1:<6} {train_aucs[i]:<12.5f} {val_aucs[i]:<12.5f} {gap:<12.5f}\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Mean':<6} {np.mean(train_aucs):<12.5f} {np.mean(val_aucs):<12.5f} {np.mean(train_aucs) - np.mean(val_aucs):<12.5f}\")\n",
        "    print(f\"{'Std':<6} {np.std(train_aucs):<12.5f} {np.std(val_aucs):<12.5f}\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "    \n",
        "    return oof_preds, test_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "02970855",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "--- Training LGB ---\n",
            "======================================================================\n",
            "\n",
            "[11:25:43] Fold 1/10 - Starting...\n",
            "[11:26:35] Fold 1/10 - Completed!\n",
            "  Fold Time: 51.3s\n",
            "  Train AUC: 0.74519 | Val AUC: 0.72731 | Gap: 0.01787\n",
            "  Progress: 1/10 (10.0%)\n",
            "  Estimated Remaining: 7.7 minutes\n",
            "  Total Elapsed: 0.9 minutes\n",
            "\n",
            "[11:26:35] Fold 2/10 - Starting...\n",
            "[11:27:31] Fold 2/10 - Completed!\n",
            "  Fold Time: 56.4s\n",
            "  Train AUC: 0.74461 | Val AUC: 0.72810 | Gap: 0.01650\n",
            "  Progress: 2/10 (20.0%)\n",
            "  Estimated Remaining: 7.2 minutes\n",
            "  Total Elapsed: 1.8 minutes\n",
            "\n",
            "[11:27:31] Fold 3/10 - Starting...\n",
            "[11:28:49] Fold 3/10 - Completed!\n",
            "  Fold Time: 78.3s\n",
            "  Train AUC: 0.75633 | Val AUC: 0.72625 | Gap: 0.03008\n",
            "  Progress: 3/10 (30.0%)\n",
            "  Estimated Remaining: 7.2 minutes\n",
            "  Total Elapsed: 3.1 minutes\n",
            "\n",
            "[11:28:49] Fold 4/10 - Starting...\n",
            "[11:30:00] Fold 4/10 - Completed!\n",
            "  Fold Time: 70.2s\n",
            "  Train AUC: 0.74880 | Val AUC: 0.72640 | Gap: 0.02240\n",
            "  Progress: 4/10 (40.0%)\n",
            "  Estimated Remaining: 6.4 minutes\n",
            "  Total Elapsed: 4.3 minutes\n",
            "\n",
            "[11:30:00] Fold 5/10 - Starting...\n",
            "[11:31:23] Fold 5/10 - Completed!\n",
            "  Fold Time: 83.5s\n",
            "  Train AUC: 0.75235 | Val AUC: 0.72753 | Gap: 0.02482\n",
            "  Progress: 5/10 (50.0%)\n",
            "  Estimated Remaining: 5.7 minutes\n",
            "  Total Elapsed: 5.7 minutes\n",
            "\n",
            "[11:31:23] Fold 6/10 - Starting...\n",
            "[11:32:51] Fold 6/10 - Completed!\n",
            "  Fold Time: 87.6s\n",
            "  Train AUC: 0.75511 | Val AUC: 0.72760 | Gap: 0.02750\n",
            "  Progress: 6/10 (60.0%)\n",
            "  Estimated Remaining: 4.7 minutes\n",
            "  Total Elapsed: 7.1 minutes\n",
            "\n",
            "[11:32:51] Fold 7/10 - Starting...\n",
            "[11:34:00] Fold 7/10 - Completed!\n",
            "  Fold Time: 68.8s\n",
            "  Train AUC: 0.75015 | Val AUC: 0.72572 | Gap: 0.02443\n",
            "  Progress: 7/10 (70.0%)\n",
            "  Estimated Remaining: 3.5 minutes\n",
            "  Total Elapsed: 8.3 minutes\n",
            "\n",
            "[11:34:00] Fold 8/10 - Starting...\n",
            "[11:35:01] Fold 8/10 - Completed!\n",
            "  Fold Time: 61.2s\n",
            "  Train AUC: 0.74354 | Val AUC: 0.73006 | Gap: 0.01348\n",
            "  Progress: 8/10 (80.0%)\n",
            "  Estimated Remaining: 2.3 minutes\n",
            "  Total Elapsed: 9.3 minutes\n",
            "\n",
            "[11:35:01] Fold 9/10 - Starting...\n",
            "[11:36:26] Fold 9/10 - Completed!\n",
            "  Fold Time: 84.9s\n",
            "  Train AUC: 0.75042 | Val AUC: 0.72948 | Gap: 0.02093\n",
            "  Progress: 9/10 (90.0%)\n",
            "  Estimated Remaining: 1.2 minutes\n",
            "  Total Elapsed: 10.7 minutes\n",
            "\n",
            "[11:36:26] Fold 10/10 - Starting...\n",
            "[11:38:03] Fold 10/10 - Completed!\n",
            "  Fold Time: 97.3s\n",
            "  Train AUC: 0.75894 | Val AUC: 0.72711 | Gap: 0.03183\n",
            "  Progress: 10/10 (100.0%)\n",
            "  Total Elapsed: 12.3 minutes\n",
            "\n",
            "======================================================================\n",
            "LGB Training Complete!\n",
            "OOF AUC: 0.72755\n",
            "Total Time: 12.3 minutes (740.2 seconds)\n",
            "Average Time per Fold: 74.0 seconds\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "AUC Summary:\n",
            "----------------------------------------------------------------------\n",
            "Fold   Train AUC    Val AUC      Gap         \n",
            "----------------------------------------------------------------------\n",
            "1      0.74519      0.72731      0.01787     \n",
            "2      0.74461      0.72810      0.01650     \n",
            "3      0.75633      0.72625      0.03008     \n",
            "4      0.74880      0.72640      0.02240     \n",
            "5      0.75235      0.72753      0.02482     \n",
            "6      0.75511      0.72760      0.02750     \n",
            "7      0.75015      0.72572      0.02443     \n",
            "8      0.74354      0.73006      0.01348     \n",
            "9      0.75042      0.72948      0.02093     \n",
            "10     0.75894      0.72711      0.03183     \n",
            "----------------------------------------------------------------------\n",
            "Mean   0.75054      0.72756      0.02299     \n",
            "Std    0.00494      0.00130     \n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "--- Training XGB ---\n",
            "======================================================================\n",
            "\n",
            "[11:38:04] Fold 1/10 - Starting...\n",
            "[11:38:59] Fold 1/10 - Completed!\n",
            "  Fold Time: 55.7s\n",
            "  Train AUC: 0.75332 | Val AUC: 0.72632 | Gap: 0.02700\n",
            "  Progress: 1/10 (10.0%)\n",
            "  Estimated Remaining: 8.4 minutes\n",
            "  Total Elapsed: 0.9 minutes\n",
            "\n",
            "[11:38:59] Fold 2/10 - Starting...\n",
            "[11:39:48] Fold 2/10 - Completed!\n",
            "  Fold Time: 48.6s\n",
            "  Train AUC: 0.75162 | Val AUC: 0.72775 | Gap: 0.02386\n",
            "  Progress: 2/10 (20.0%)\n",
            "  Estimated Remaining: 7.0 minutes\n",
            "  Total Elapsed: 1.7 minutes\n",
            "\n",
            "[11:39:48] Fold 3/10 - Starting...\n",
            "[11:40:47] Fold 3/10 - Completed!\n",
            "  Fold Time: 58.7s\n",
            "  Train AUC: 0.75716 | Val AUC: 0.72498 | Gap: 0.03218\n",
            "  Progress: 3/10 (30.0%)\n",
            "  Estimated Remaining: 6.3 minutes\n",
            "  Total Elapsed: 2.7 minutes\n",
            "\n",
            "[11:40:47] Fold 4/10 - Starting...\n",
            "[11:41:50] Fold 4/10 - Completed!\n",
            "  Fold Time: 63.5s\n",
            "  Train AUC: 0.75820 | Val AUC: 0.72515 | Gap: 0.03305\n",
            "  Progress: 4/10 (40.0%)\n",
            "  Estimated Remaining: 5.7 minutes\n",
            "  Total Elapsed: 3.8 minutes\n",
            "\n",
            "[11:41:50] Fold 5/10 - Starting...\n",
            "[11:43:21] Fold 5/10 - Completed!\n",
            "  Fold Time: 90.3s\n",
            "  Train AUC: 0.75822 | Val AUC: 0.72626 | Gap: 0.03196\n",
            "  Progress: 5/10 (50.0%)\n",
            "  Estimated Remaining: 5.3 minutes\n",
            "  Total Elapsed: 5.3 minutes\n",
            "\n",
            "[11:43:21] Fold 6/10 - Starting...\n",
            "[11:44:48] Fold 6/10 - Completed!\n",
            "  Fold Time: 87.4s\n",
            "  Train AUC: 0.76163 | Val AUC: 0.72684 | Gap: 0.03479\n",
            "  Progress: 6/10 (60.0%)\n",
            "  Estimated Remaining: 4.5 minutes\n",
            "  Total Elapsed: 6.7 minutes\n",
            "\n",
            "[11:44:48] Fold 7/10 - Starting...\n",
            "[11:45:49] Fold 7/10 - Completed!\n",
            "  Fold Time: 61.2s\n",
            "  Train AUC: 0.75296 | Val AUC: 0.72527 | Gap: 0.02769\n",
            "  Progress: 7/10 (70.0%)\n",
            "  Estimated Remaining: 3.3 minutes\n",
            "  Total Elapsed: 7.8 minutes\n",
            "\n",
            "[11:45:49] Fold 8/10 - Starting...\n",
            "[11:46:54] Fold 8/10 - Completed!\n",
            "  Fold Time: 64.4s\n",
            "  Train AUC: 0.75854 | Val AUC: 0.72970 | Gap: 0.02884\n",
            "  Progress: 8/10 (80.0%)\n",
            "  Estimated Remaining: 2.2 minutes\n",
            "  Total Elapsed: 8.8 minutes\n",
            "\n",
            "[11:46:54] Fold 9/10 - Starting...\n",
            "[11:47:50] Fold 9/10 - Completed!\n",
            "  Fold Time: 56.4s\n",
            "  Train AUC: 0.75351 | Val AUC: 0.72869 | Gap: 0.02482\n",
            "  Progress: 9/10 (90.0%)\n",
            "  Estimated Remaining: 1.1 minutes\n",
            "  Total Elapsed: 9.8 minutes\n",
            "\n",
            "[11:47:50] Fold 10/10 - Starting...\n",
            "[11:48:48] Fold 10/10 - Completed!\n",
            "  Fold Time: 57.6s\n",
            "  Train AUC: 0.75598 | Val AUC: 0.72622 | Gap: 0.02976\n",
            "  Progress: 10/10 (100.0%)\n",
            "  Total Elapsed: 10.7 minutes\n",
            "\n",
            "======================================================================\n",
            "XGB Training Complete!\n",
            "OOF AUC: 0.72671\n",
            "Total Time: 10.7 minutes (644.2 seconds)\n",
            "Average Time per Fold: 64.4 seconds\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "AUC Summary:\n",
            "----------------------------------------------------------------------\n",
            "Fold   Train AUC    Val AUC      Gap         \n",
            "----------------------------------------------------------------------\n",
            "1      0.75332      0.72632      0.02700     \n",
            "2      0.75162      0.72775      0.02386     \n",
            "3      0.75716      0.72498      0.03218     \n",
            "4      0.75820      0.72515      0.03305     \n",
            "5      0.75822      0.72626      0.03196     \n",
            "6      0.76163      0.72684      0.03479     \n",
            "7      0.75296      0.72527      0.02769     \n",
            "8      0.75854      0.72970      0.02884     \n",
            "9      0.75351      0.72869      0.02482     \n",
            "10     0.75598      0.72622      0.02976     \n",
            "----------------------------------------------------------------------\n",
            "Mean   0.75611      0.72672      0.02939     \n",
            "Std    0.00302      0.00149     \n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration (Optimized for Speed) ---\n",
        "\n",
        "# LightGBM: Optimized for faster training\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'learning_rate': 0.05,  # Increased from 0.02 for faster convergence (2.5x faster)\n",
        "    'num_leaves': 31,       # Slightly smaller leaves to prevent Overfitting\n",
        "    'n_estimators': 2000,   # Reduced from 5000 (controlled by Early Stopping)\n",
        "    'colsample_bytree': 0.7,\n",
        "    'subsample': 0.7,\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "# XGBoost: Optimized for faster training\n",
        "xgb_params = {\n",
        "    'n_estimators': 2000,   # Reduced from 5000\n",
        "    'learning_rate': 0.05,  # Increased from 0.02 for faster convergence (2.5x faster)\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'eval_metric': 'auc',\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'tree_method': 'hist',  # Faster training\n",
        "    'early_stopping_rounds': 50  # Reduced from 100 for faster stopping\n",
        "}\n",
        "\n",
        "# --- Execution ---\n",
        "oof_lgb, pred_lgb = train_model('lgb', X, y, test, lgb_params)\n",
        "oof_xgb, pred_xgb = train_model('xgb', X, y, test, xgb_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "98d8b4c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Ensembling LightGBM and XGBoost\n",
            "======================================================================\n",
            "\n",
            "Individual Model Performance:\n",
            "  LightGBM OOF AUC: 0.72755\n",
            "  XGBoost OOF AUC:  0.72671\n",
            "\n",
            "======================================================================\n",
            "Testing Different Weight Combinations\n",
            "======================================================================\n",
            "\n",
            "Top 5 Weight Combinations:\n",
            "Rank   LGB Weight   XGB Weight   OOF AUC     \n",
            "--------------------------------------------------\n",
            "1      0.65         0.35         0.72792      <-- BEST\n",
            "2      0.60         0.40         0.72792     \n",
            "3      0.70         0.30         0.72791     \n",
            "4      0.55         0.45         0.72790     \n",
            "5      0.75         0.25         0.72789     \n",
            "\n",
            "======================================================================\n",
            "Final Results\n",
            "======================================================================\n",
            "Best Method: Ensemble (LGB: 0.65, XGB: 0.35)\n",
            "Final OOF AUC: 0.72792\n",
            "Best Single Model AUC: 0.72755\n",
            "Improvement: 0.00037\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Submission File Created\n",
            "======================================================================\n",
            "File saved to: /workspace/competitions/playground-series-s5e12/submissions/ensemble_submission.csv\n",
            "Shape: (300000, 2)\n",
            "Prediction range: [0.06720, 0.99037]\n",
            "Mean prediction: 0.62253\n",
            "Method used: Ensemble (LGB: 0.65, XGB: 0.35)\n",
            "OOF AUC: 0.72792\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# --- Ensemble: Find Best Combination and Create Submission ---\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Ensembling LightGBM and XGBoost\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calculate OOF AUC for each model\n",
        "lgb_oof_auc = roc_auc_score(y, oof_lgb)\n",
        "xgb_oof_auc = roc_auc_score(y, oof_xgb)\n",
        "\n",
        "print(f\"\\nIndividual Model Performance:\")\n",
        "print(f\"  LightGBM OOF AUC: {lgb_oof_auc:.5f}\")\n",
        "print(f\"  XGBoost OOF AUC:  {xgb_oof_auc:.5f}\")\n",
        "\n",
        "# Test different weight combinations to find the best\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Testing Different Weight Combinations\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "best_auc = max(lgb_oof_auc, xgb_oof_auc)\n",
        "best_weights = None\n",
        "best_method = \"Single Model (Best Individual)\"\n",
        "\n",
        "# Test different weight combinations (more granular search)\n",
        "weight_combinations = []\n",
        "for lgb_w in np.arange(0.0, 1.01, 0.05):  # 0.0 to 1.0 in steps of 0.05\n",
        "    xgb_w = 1.0 - lgb_w\n",
        "    ensemble_oof_test = lgb_w * oof_lgb + xgb_w * oof_xgb\n",
        "    ensemble_auc_test = roc_auc_score(y, ensemble_oof_test)\n",
        "    weight_combinations.append({\n",
        "        'lgb_weight': lgb_w,\n",
        "        'xgb_weight': xgb_w,\n",
        "        'auc': ensemble_auc_test\n",
        "    })\n",
        "    \n",
        "    if ensemble_auc_test > best_auc:\n",
        "        best_auc = ensemble_auc_test\n",
        "        best_weights = (lgb_w, xgb_w)\n",
        "        best_method = f\"Ensemble (LGB: {lgb_w:.2f}, XGB: {xgb_w:.2f})\"\n",
        "\n",
        "# Sort by AUC to show top combinations\n",
        "weight_combinations.sort(key=lambda x: x['auc'], reverse=True)\n",
        "\n",
        "print(f\"\\nTop 5 Weight Combinations:\")\n",
        "print(f\"{'Rank':<6} {'LGB Weight':<12} {'XGB Weight':<12} {'OOF AUC':<12}\")\n",
        "print(\"-\" * 50)\n",
        "for i, combo in enumerate(weight_combinations[:5], 1):\n",
        "    marker = \" <-- BEST\" if combo['auc'] == best_auc else \"\"\n",
        "    print(f\"{i:<6} {combo['lgb_weight']:<12.2f} {combo['xgb_weight']:<12.2f} {combo['auc']:<12.5f}{marker}\")\n",
        "\n",
        "# Use the best combination for final predictions\n",
        "if best_weights:\n",
        "    final_pred = best_weights[0] * pred_lgb + best_weights[1] * pred_xgb\n",
        "    final_oof = best_weights[0] * oof_lgb + best_weights[1] * oof_xgb\n",
        "    final_auc = best_auc\n",
        "else:\n",
        "    # Use the best single model if no improvement\n",
        "    if lgb_oof_auc > xgb_oof_auc:\n",
        "        final_pred = pred_lgb\n",
        "        final_oof = oof_lgb\n",
        "        final_auc = lgb_oof_auc\n",
        "        best_method = \"LightGBM (Best Single)\"\n",
        "    else:\n",
        "        final_pred = pred_xgb\n",
        "        final_oof = oof_xgb\n",
        "        final_auc = xgb_oof_auc\n",
        "        best_method = \"XGBoost (Best Single)\"\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Final Results\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Best Method: {best_method}\")\n",
        "print(f\"Final OOF AUC: {final_auc:.5f}\")\n",
        "print(f\"Best Single Model AUC: {max(lgb_oof_auc, xgb_oof_auc):.5f}\")\n",
        "if best_weights:\n",
        "    print(f\"Improvement: {final_auc - max(lgb_oof_auc, xgb_oof_auc):.5f}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Create submission file\n",
        "import os\n",
        "submission_dir = '/workspace/competitions/playground-series-s5e12/submissions'\n",
        "os.makedirs(submission_dir, exist_ok=True)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'diagnosed_diabetes': final_pred\n",
        "})\n",
        "\n",
        "submission_path = os.path.join(submission_dir, 'ensemble_submission.csv')\n",
        "submission.to_csv(submission_path, index=False)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Submission File Created\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"File saved to: {submission_path}\")\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "print(f\"Prediction range: [{submission['diagnosed_diabetes'].min():.5f}, {submission['diagnosed_diabetes'].max():.5f}]\")\n",
        "print(f\"Mean prediction: {submission['diagnosed_diabetes'].mean():.5f}\")\n",
        "print(f\"Method used: {best_method}\")\n",
        "print(f\"OOF AUC: {final_auc:.5f}\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
